---
date: 2025-09-18
tags:
aliases:
image: "![[many_attention_1.png]]"
---
옛날 옛적에 RNN이 살았습니다. 다들 알다시피 RNN은 시계열 분석이나 자연어 처리에 사용하는 딥러닝 모델입니다. 기억력이라는 것이 있었기 때문입니다.  이제 용어는 익숙한 어텐션, 얼마나 알고있나요? 오늘은 어텐션이 뭔지 확실히 짚고 가겠습니다. 게다가 어텐션이라고 다 같은 어텐션이 아닙니다. 어텐션에도 여러 종류와 방식이 있습니다. 오늘은 이런 어텐션 메커니즘들을 알아봅니다. 

![[many_attention_1.png]]
## 어텐션이란?

ChatGPT나 Claude 같은 AI가 우리 말을 이해하는 비밀, 바로 '어텐션'에 있다는 걸 아시나요? 하지만 어텐션이라고 다 같은 어텐션이 아닙니다. 마치 요리에 다양한 조리법이 있듯이, 어텐션에도 여러 종류와 방식이 있거든요. 오늘은 이런 어텐션 메커니즘들을 쉽게 풀어서 설명해드리겠습니다.

## 어텐션이 뭐길래?

먼저 어텐션이 무엇인지 간단히 이해해보겠습니다. 사람이 책을 읽을 때를 생각해보세요. "그는 공원에 갔다. 그곳은 정말 아름다웠다"라는 문장에서 '그곳'이 '공원'을 가리킨다는 걸 우리는 자연스럽게 알 수 있죠. 어텐션은 AI가 이런 식으로 문장 내 단어들 간의 관계를 파악하는 메커니즘입니다.

## 기본 어텐션 함수들: 점수 계산하는 방법들의 진화

### Additive Attention

**배경**: 2014년 Bahdanau 등이 기계 번역에서 RNN의 한계를 극복하기 위해 개발했습니다. 당시 RNN 인코더-디코더는 긴 문장을 번역할 때 앞부분 정보를 잊어버리는 문제가 심각했어요.

**핵심 아이디어**: "디코더가 번역할 때 인코더의 모든 시점을 다시 볼 수 있게 하자!"

두 단어가 얼마나 관련이 있는지 점수를 계산할 때, 두 단어의 정보를 더한 다음 작은 신경망(피드포워드)으로 처리합니다:

$e_{ij} = v^T \tanh(W_q q_i + W_k k_j)$

쉽게 말하면 "두 단어 정보를 섞어서 작은 네트워크가 관계도를 판단하게 하자"는 방식이에요. 혁신적이었지만 계산이 복잡했습니다.

### Multiplicative Attention

**배경**: Luong 등이 "굳이 신경망을 써야 할까?"라고 의문을 가지며 더 간단한 방법을 제안했습니다. 당시 GPU가 행렬 곱셈에 최적화되어 있다는 점에 착안했어요.

**핵심 차이점**: 복잡한 신경망 대신 단순한 내적(곱셈)으로 유사도를 계산합니다:

$e_{ij} = q_i^T k_j$

**장점**: 계산이 훨씬 빠르고 GPU 친화적 **단점**: 표현력이 다소 제한적

실제로 성능은 비슷하면서 속도가 월등히 빨라서 큰 주목을 받았습니다.

### Scaled Dot-Product Attention

**배경**: Transformer 연구진이 곱하기 어텐션을 사용하다가 발견한 문제점을 해결하기 위해 개발했습니다. 차원이 클 때 내적 결과가 너무 커져서 소프트맥스가 극단값으로 치우치는 현상을 발견했어요.

**구체적 문제**: 512차원에서는 괜찮았는데, 1024차원으로 늘리니까 어텐션 가중치가 거의 0 아니면 1로만 나오는 거예요!

**해결책**: $\sqrt{d_k}$로 나누어 스케일을 조정:

$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$

**왜 제곱근인가?**: 수학적으로 내적의 분산이 차원에 비례하므로, 표준편차인 $\sqrt{d_k}$로 정규화하면 분산이 일정해집니다.

이 작은 변화가 현재 모든 대규모 언어 모델의 기반이 되었어요!

## 어텐션이 보는 범위에 따른 분류

### Self-Attention

**배경**: 처음에는 문장 요약이나 독해에서 "한 문장 내에서도 단어들끼리 서로 영향을 주지 않을까?"라는 아이디어에서 시작되었습니다. 특히 "The animal didn't cross the street because it was too tired"에서 'it'이 'animal'을 가리킨다는 걸 모델이 스스로 알 수 있게 하려는 것이었어요.

**혁신적인 이유**: 기존에는 RNN이 순차적으로 처리하면서 이런 관계를 암묵적으로 학습했는데, Self-Attention은 **명시적으로** 모든 단어 쌍의 관계를 계산합니다.

**실제 예시**: "철수는 학교에 갔다. 그는 친구를 만났다"

- 기존 RNN: 순차 처리로 '그는'을 만날 때 '철수' 정보가 희미해짐
- Self-Attention: '그는'이 모든 이전 단어들과 직접 연결되어 '철수'와 강한 관계 발견

### Cross-Attention

**원래 목적**: 기계 번역에서 "번역할 때 원문의 어떤 부분을 봐야 할까?"를 해결하기 위해 탄생했습니다.

**구체적 문제**: "I love you"를 "나는 너를 사랑한다"로 번역할 때

- 기존 방식: 전체 영어 문장을 하나의 벡터로 압축 → 정보 손실
- Cross-Attention: 각 한국어 단어를 생성할 때마다 관련된 영어 단어에 집중

**현재 활용**: 인코더-디코더 구조에서 핵심 역할

- 인코더: "I love you" 분석
- 디코더: "나는" 생성 시 "I"에 어텐션, "사랑한다" 생성 시 "love"에 어텐션

## 헤드 개수로 나누는 방법들

### Single-Head Attention

**한계점 발견**: 연구자들이 Single-Head로 실험하다가 이상한 현상을 발견했습니다. 모델이 한 번에 하나의 관점에서만 문장을 보는 거예요.

예를 들어 "The cat sat on the mat"에서:

- 어떨 때는 주어-동사 관계 (cat-sat)에만 집중
- 어떨 때는 전치사 관계 (on-mat)에만 집중
- 동시에 여러 관계를 보지 못하는 한계

**비유**: 마치 한 쪽 눈으로만 사물을 보는 것처럼, 깊이감과 입체적 이해가 부족했습니다.

### Multi-Head Attention

**핵심 통찰**: "사람도 문장을 읽을 때 여러 관점에서 동시에 이해하지 않나?"

Transformer 연구진의 실험 결과:

- **8개 헤드**가 각각 다른 역할을 자연스럽게 학습
- 헤드별 전문화가 명확하게 나타남

**실제 헤드별 역할 분석** (논문에서 발견한 실제 패턴):

**1번 헤드**: "주어와 동사 찾기"

- "The cat **sat**" → cat과 sat 사이에 강한 연결

**2번 헤드**: "수식 관계 파악"

- "big red **car**" → big, red 모두 car에 어텐션

**3번 헤드**: "장거리 의존성 추적"

- "The cat, which was sleeping, **woke up**" → cat과 woke up 연결

**4번 헤드**: "대명사 해결"

- "John went home. **He** was tired" → He와 John 강하게 연결

**5번 헤드**: "구문 구조"

- 괄호나 인용부호 같은 문법적 경계 감지

**왜 8개인가?**: 실험적으로 찾은 최적값

- 4개: 너무 적어서 복잡한 관계 놓침
- 16개: 너무 많아서 중복되고 계산 비효율
- 8개: 성능과 효율의 균형점

**계산 비용의 놀라운 사실**: 8개 헤드를 써도 전체 계산량은 단일 헤드와 거의 동일합니다. 각 헤드의 차원을 1/8로 줄이기 때문이에요!

## 효율성을 위한 특수한 어텐션들

### Local Attention

**문제 인식**: 연구자들이 긴 문서(1000단어 이상)로 실험하다가 발견한 치명적 문제

- 계산량이 문장 길이의 **제곱**에 비례 증가
- 1000단어 문서 → 100만 번의 어텐션 계산!
- GPU 메모리 부족으로 학습 불가

**핵심 아이디어**: "멀리 있는 단어와 직접적 관련성이 정말 필요할까?"

**실제 구현**: 현재 단어 주변의 일정 범위(보통 ±10 단어) 내에서만 어텐션 계산

**효과**:

- 계산량: $O(n^2) → O(n×w)$ (w는 윈도우 크기)
- 1000단어 문서: 100만 → 2만 번 계산 (98% 감소!)
- 성능 손실: 거의 없음 (대부분 중요한 관계는 근처에 있음)

### Sparse Attention

**배경**: GPT-2 개발 중 "모든 단어 쌍을 다 볼 필요가 있을까?"라는 근본적 질문

- 1024 토큰 입력: 100만 개 어텐션 연결
- 실제 의미 있는 연결: 10% 미만

**창의적 해결책**: 전략적으로 선택된 패턴만 계산

**패턴 예시**:

1. **Strided Pattern**: 일정 간격으로 건너뛰며 어텐션 (1, 3, 5, 7번째 단어...)
2. **Fixed Pattern**: 문장 시작/끝 같은 중요 위치는 항상 연결
3. **Random Pattern**: 일부는 무작위로 연결하여 다양성 확보

**실제 성능**: GPT-3에서 이 기법으로 메모리 사용량 1/10 감소!

### Linear Attention

**동기**: "어텐션의 본질을 유지하면서 계산량을 근본적으로 줄일 수 없을까?"

**수학적 통찰**: 어텐션 계산 순서를 바꾸는 천재적 아이디어

- 기존: $\text{softmax}(QK^T)V$ → $O(n^2d)$
- 선형: $Q(\text{something}(K^TV))$ → $O(nd^2)$

**언제 유리한가?**:

- 문장이 매우 긴 경우 (n > d)
- 실시간 처리가 필요한 경우 (스트리밍)

**트레이드오프**: 계산 속도 ↑, 표현력 약간 ↓

## 시간의 흐름을 고려한 분류

### Bidirectional Attention

**기존 상식에 대한 도전**: "문장을 이해할 때 꼭 왼쪽부터 읽어야 할까?"

**BERT 이전의 세상**:

- 모든 언어 모델은 왼쪽→오른쪽 순서로만 처리
- "The cat is ___"에서 빈칸을 채우려면 앞의 정보만 사용

**BERT의 파격적 아이디어**:

- "빈칸 맞추기 게임을 해보자!" (MLM: Masked Language Modeling)
- "The ___ is sleeping" → 앞뒤 맥락 모두 보고 빈칸 추론

**실제 차이**:

```
문장: "The movie was really ___. I loved it."

- 일방향: "The movie was really" → 예측 어려움
- 양방향: "The movie was really ___. I loved it." → "good/great" 쉽게 예측
```

**결과**: BERT가 11개 자연어처리 태스크에서 기존 기록 모두 갱신!

**언제 사용?**:

- 텍스트 분류, 감정 분석, 질의응답
- 전체 문맥을 다 알고 있는 상황

### Causal Attention

**반대 철학**: "언어 생성은 본질적으로 순차적이다"

**핵심 제약**: "미래는 볼 수 없다"

- 문장 생성 중에는 아직 만들어지지 않은 단어를 미리 볼 수 없음
- 마스킹으로 미래 정보를 물리적으로 차단

**실제 구현**:

```
입력: "The cat sat on"
생성 과정:
1. "The" → 다음 단어 예측
2. "The cat" → 다음 단어 예측  
3. "The cat sat" → 다음 단어 예측
4. "The cat sat on" → 다음 단어 예측
```

각 단계에서 이전 단어들만 보고 다음 단어를 예측합니다.

**마스킹 메커니즘**:

- 어텐션 행렬에서 미래 위치를 -∞로 설정
- 소프트맥스 후에는 0이 되어 완전 차단

**언제 사용?**:

- 텍스트 생성 (GPT, ChatGPT)
- 대화 시스템, 요약, 번역의 디코더 부분

### 두 방식의 근본적 차이

|특성|Bidirectional|Causal|
|---|---|---|
|**목적**|텍스트 이해|텍스트 생성|
|**정보 접근**|전체 문맥|과거만|
|**대표 모델**|BERT, RoBERTa|GPT, ChatGPT|
|**장점**|깊은 이해|자연스러운 생성|

## 어텐션들이 협력하는 모습

현실의 Transformer에서는 이런 다양한 어텐션들이 협력해서 작동합니다:

1. **인코더에서**: Multi-Head Self-Attention으로 입력 문장 분석
2. **디코더에서**: Masked Multi-Head Self-Attention으로 생성 중인 문장 분석
3. **인코더-디코더 사이**: Multi-Head Cross-Attention으로 두 정보 연결

마치 오케스트라에서 각 악기가 다른 역할을 하면서도 하나의 아름다운 선율을 만들어내는 것처럼요.

## 마무리하며

어텐션 메커니즘은 단순해 보이지만, 실제로는 이렇게 다양한 변형과 조합이 가능한 깊이 있는 기술입니다. 각각의 방식은 서로 다른 상황과 목적에 최적화되어 있고, 현재의 AI 시스템들은 이런 다양한 어텐션 메커니즘들을 조합해서 사용하고 있어요.

다음번에 AI와 대화할 때는 "아, 지금 이 AI가 내 말의 여러 부분에 각각 다른 방식으로 어텐션을 주고 있구나"라고 생각해보시면 어떨까요? 조금 더 신기하게 느껴질 거예요.