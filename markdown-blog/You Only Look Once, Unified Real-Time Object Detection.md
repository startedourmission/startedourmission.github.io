---
date: 2025-06-30
tags:
  - 논문
  - 딥러닝
  - 영상처리
  - 객체탐지
aliases:
---
- **발표연도:** 2016
- **저널:** IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2016
- **저자:** Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi
- **기관 및 대학:** University of Washington, Allen Institute for AI, Facebook AI Research
## YOLO 소개: 객체 탐지에 대한 통합 접근 방식

객체 탐지—이미지 내에서 객체를 식별하고 위치를 파악하는 작업—는 오랫동안 컴퓨터 비전의 핵심 과제였습니다. 2015년 이전에는 지배적인 접근 방식들이 복잡하고 다단계적인 파이프라인을 요구했으며, 이는 계산 비용이 많이 들고 실시간 애플리케이션에는 너무 느렸습니다. Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi가 저술한 YOLO(You Only Look Once) 논문은 객체 탐지를 다단계 분류 작업이 아닌 단일 회귀 문제로 재구성함으로써 패러다임의 전환을 가져왔습니다.

![YOLO 시스템 개요](https://paper-assets.alphaxiv.org/figures/1506.02640v5/img-0.jpeg "YOLO 시스템 개요")_그림 1: YOLO 탐지 시스템은 세 가지 간단한 단계로 이미지를 처리합니다: (1) 입력 이미지 크기 조정, (2) 단일 컨볼루션 네트워크 실행, (3) 비최대 억제 적용하여 최종 탐지 생성._

YOLO의 핵심 혁신은 단일 평가에서 전체 이미지로부터 바운딩 박스와 클래스 확률을 직접 예측하는 통합 아키텍처에 있습니다. 이 접근 방식은 처음으로 실시간 객체 탐지를 가능하게 했으며, 경쟁력 있는 정확도를 유지하면서 초당 45-155프레임의 속도를 달성했습니다. 이 작업은 연구자들이 객체 탐지에 접근하는 방식을 근본적으로 변화시켰고, 속도와 정확도 사이의 균형을 우선시하는 새로운 세대의 단일 샷 탐지기들을 탄생시켰습니다.

## 배경 및 동기

YOLO 이전에 객체 탐지 시스템은 일반적으로 두 가지 패러다임 중 하나를 따랐습니다. DPM(Deformable Parts Models)과 같은 슬라이딩 윈도우 접근 방식은 이미지의 다양한 위치와 스케일에서 분류기를 체계적으로 평가했습니다. 개념적으로는 간단했지만, 이러한 방법들은 중복 영역에 대한 여러 평가를 필요로 했기 때문에 계산적으로 중복되고 느렸습니다.

당시 더 성공적인 접근 방식은 R-CNN 및 그 변형(Fast R-CNN, Faster R-CNN)이 개척한 영역 제안(region proposal) 방법이었습니다. 이 시스템들은 먼저 Selective Search와 같은 알고리즘을 사용하여 잠재적인 객체 위치를 생성한 다음, 컨볼루션 신경망을 사용하여 제안된 각 영역을 분류했습니다. 속도와 정확도 면에서 개선이 있었음에도 불구하고, 이 방법들은 본질적으로 다단계적 특성으로 인해 여전히 한계가 있었으며, 종종 이미지당 몇 초가 소요되고 복잡한 최적화 절차를 요구했습니다.

YOLO는 스트리밍 비디오 애플리케이션에 적합한 실시간 성능 달성, 통합된 엔드-투-엔드 학습 가능 시스템 구축, 이미지 내 객체에 대한 전역적 맥락 추론 가능이라는 몇 가지 핵심 목표에 의해 동기 부여되었습니다. 저자들은 기존 방법들이 순차적인 처리 단계로 인해 높은 지연 시간을 겪었고, 더 넓은 맥락 없이 고립된 이미지 영역만 처리했기 때문에 종종 배경 분류 오류를 범한다고 관찰했습니다.

## YOLO 아키텍처 및 방법론

YOLO의 핵심 혁신은 객체 탐지에 대한 그리드 기반 접근 방식에 있습니다. 이 시스템은 각 입력 이미지를 S×SS×S 그리드(일반적으로 7×77×7)로 나누며, 각 그리드 셀은 중심이 해당 셀 내에 떨어지는 객체를 탐지하는 책임을 집니다.

![YOLO 탐지 시스템](https://paper-assets.alphaxiv.org/figures/1506.02640v5/img-1.jpeg "YOLO 탐지 시스템")_그림 2: YOLO 모델은 입력 이미지를 그리드로 나누고, 각 그리드 셀에 대해 바운딩 박스, 신뢰도 점수, 클래스 확률을 동시에 예측하여 통합된 텐서 출력을 생성합니다._

각 그리드 셀은 여러 구성 요소를 동시에 예측합니다.

**경계 상자 예측**: 각 셀은 BB개의 경계 상자(일반적으로 2개)를 예측하며, 각 상자는 다섯 가지 값: (x,y,w,h,confidence)(x,y,w,h,confidence)로 구성됩니다. 좌표 (x,y)(x,y)는 그리드 셀 경계에 대한 상대적인 상자 중심을 0과 1 사이로 정규화하여 나타냅니다. 치수 (w,h)(w,h)는 전체 이미지에 대한 상자의 너비와 높이를 나타냅니다. 신뢰도 점수는 상자에 객체가 포함될 확률과 경계 상자의 정확도를 모두 반영하며, 공식적으로는 Pr(Object)×IOUpredtruthPr(Object)×IOUpredtruth​로 정의됩니다.

**클래스 확률**: 각 그리드 셀은 또한 CC개의 조건부 클래스 확률 Pr(Classi∣Object)Pr(Classi​∣Object)을 예측하는데, 이는 셀에 객체가 존재할 경우 각 클래스의 가능성을 나타냅니다.

**최종 감지 점수**: 테스트 시, 클래스별 신뢰도 점수는 조건부 클래스 확률에 개별 상자 신뢰도 예측을 곱하여 계산됩니다.

Pr(Classi∣Object)×Pr(Object)×IOUpredtruth=Pr(Classi)×IOUpredtruthPr(Classi​∣Object)×Pr(Object)×IOUpredtruth​=Pr(Classi​)×IOUpredtruth​

S=7S=7, B=2B=2, C=20C=20 클래스를 가진 PASCAL VOC 데이터셋의 경우, 이는 7×7×307×7×30 출력 텐서를 생성합니다.

![YOLO 네트워크 아키텍처](https://paper-assets.alphaxiv.org/figures/1506.02640v5/img-2.jpeg "YOLO 네트워크 아키텍처")_그림 3: YOLO 네트워크 아키텍처는 24개의 컨볼루션 레이어와 2개의 완전 연결 레이어로 구성되며, GoogLeNet에서 영감을 받은 1×1 차원 감소 레이어와 3×3 컨볼루션 레이어가 번갈아 사용됩니다._

**네트워크 설계**: YOLO는 GoogLeNet에서 영감을 받은 단일 컨볼루션 신경망을 사용하며, 24개의 컨볼루션 레이어와 2개의 완전 연결 레이어로 구성됩니다. 이 아키텍처는 계산 부담을 줄이기 위해 3×3 컨볼루션 레이어 이전에 1×1 차원 감소 레이어를 사용합니다. "Fast YOLO"라는 더 빠른 변형은 더 높은 속도를 위해 더 적은 필터와 단 9개의 컨볼루션 레이어만 사용합니다.

**훈련 과정**: 네트워크 훈련은 여러 핵심 단계를 포함합니다.

1. **사전 훈련**: 처음 20개의 컨볼루션 레이어는 ImageNet에서 224×224224×224 해상도로 사전 훈련됩니다.
2. **감지 미세 조정**: 4개의 추가 컨볼루션 레이어와 2개의 완전 연결 레이어가 추가되고, 입력 해상도는 448×448448×448로 증가됩니다.
3. **손실 함수**: YOLO는 순진한 제곱 오차의 한계를 해결하기 위해 여러 수정 사항이 적용된 신중하게 설계된 제곱합 오차 손실을 사용합니다.

$$
λcoord∑i=0S2∑j=0B1ijobj[(xi−x^i)2+(yi−y^i)2]λcoord​i=0∑S2​j=0∑B​1ijobj​[(xi​−x^i​)2+(yi​−y^​i​)2]
$$
$$
+λcoord∑i=0S2∑j=0B1ijobj[(wi−w^i)2+(hi−h^i)2]+λcoord​i=0∑S2​j=0∑B​1ijobj​[(wi​​−w^i​​)2+(hi​​−h^i​​)2]
$$

손실 함수는 가중치 항(λcoord=5λcoord​=5, λnoobj=0.5λnoobj​=0.5)을 사용하여 위치 및 분류 오차의 균형을 맞추고, 상자 치수의 제곱근을 예측하여 다양한 객체 크기를 더 잘 처리합니다.

## 실험 결과 및 성능 분석

YOLO는 표준 벤치마크에서 경쟁력 있는 정확도를 유지하면서 전례 없는 속도를 보여주었습니다. PASCAL VOC 2007에서 YOLO는 45 FPS로 이미지를 처리하면서 63.4%의 평균 정밀도(mAP)를 달성했으며, Fast YOLO는 155 FPS에서 52.7% mAP를 기록했습니다. 이러한 정확도는 Fast R-CNN(70.0% mAP)과 같은 더 느린 최첨단 방법보다는 낮았지만, 다른 실시간 시스템에 비해 상당한 개선을 나타냈습니다.

![오류 분석 비교](https://paper-assets.alphaxiv.org/figures/1506.02640v5/img-3.jpeg "오류 분석 비교")_그림 4: YOLO와 Fast R-CNN을 비교한 오류 분석은 보완적인 오류 프로필을 보여줍니다. YOLO는 더 많은 위치 오차를 범하지만 배경 오차는 현저히 적은 반면, Fast R-CNN은 더 나은 위치 파악 능력을 가지지만 배경 영역에서 더 많은 오탐을 생성합니다._

오류 분석을 통해 YOLO의 성능 특성에 대한 중요한 통찰력을 얻었습니다. Fast R-CNN과 비교했을 때, YOLO는 훨씬 더 많은 지역화 오류(19.0% 대 8.6%)를 범했지만, 배경 오류는 훨씬 적었습니다(4.75% 대 13.6%). 이러한 상호 보완적인 오류 프로파일은 YOLO의 전역 컨텍스트 추론이 배경 패치를 객체로 오인하는 것을 피하는 데 도움이 되었지만, 거친 그리드 시스템이 정밀한 지역화를 제한했음을 시사했습니다.

**모델 결합의 이점**: 이러한 상호 보완적인 강점을 활용하여 저자들은 YOLO와 Fast R-CNN을 결합하면 성능이 크게 향상된다는 것을 입증했습니다. YOLO를 사용하여 Fast R-CNN의 배경 탐지를 제거함으로써 Fast R-CNN의 VOC 2007 mAP가 71.8%에서 75.0%로 향상되었습니다. 이는 Fast R-CNN의 다른 변형들을 결합하여 얻은 이득을 훨씬 뛰어넘는 상당한 3.2% 개선입니다.

**일반화 성능**: YOLO는 예술 작품 데이터셋에서 테스트했을 때 우수한 일반화 능력을 보여주었습니다. R-CNN의 성능은 피카소 그림에 적용했을 때 극적으로 저하되었지만(AP 54.2%에서 10.4%로), YOLO는 훨씬 더 안정적인 성능을 유지했습니다(AP 59.2%에서 53.3%로).

![Generalization Results](https://paper-assets.alphaxiv.org/figures/1506.02640v5/img-4.jpeg "Generalization Results")_그림 5: 피카소 데이터셋의 정밀도-재현율 곡선은 YOLO가 다른 탐지 방법에 비해 우수한 일반화 능력을 보여주며, 예술적 이미지에 적용했을 때 성능을 훨씬 더 잘 유지함을 입증합니다._

이러한 견고성은 YOLO가 전역 이미지 처리 방식을 통해 더 일반화 가능한 객체 표현을 학습할 수 있는 능력에서 비롯되었으며, 픽셀 수준의 외형은 변하지만 객체 형태와 공간 관계는 일관되게 유지되는 도메인 변화에 덜 민감하게 만들었습니다.

## 기술적 한계 및 과제

장점에도 불구하고, YOLO는 그리드 기반 설계로 인해 몇 가지 내재적인 한계에 직면했습니다. 그리드 시스템의 공간적 제약은 서로 가깝게 나타나는 작은 객체를 탐지하는 능력을 제한했습니다. 각 그리드 셀은 고정된 수의 경계 상자와 하나의 클래스 확률 세트만 예측할 수 있었기 때문입니다. 이는 특히 새 떼나 작은 객체 그룹과 같은 범주에서 성능에 영향을 미쳤습니다.

예측의 거친 공간 해상도 또한 지역화 오류에 기여했으며, 특히 작은 객체의 경우 더욱 그러했습니다. 경계 상자 예측이 그리드 수준에서 이루어지고 네트워크에 상당한 다운샘플링이 포함되었기 때문에, 더 높은 해상도로 객체를 검사할 수 있는 방법에 비해 미세한 지역화는 어려움이 입증되었습니다.

또한, YOLO는 훈련 데이터에서 경계 상자 예측을 전적으로 학습했기 때문에 훈련 세트에 잘 표현되지 않은 비정상적인 종횡비 또는 구성을 가진 객체에 민감할 수 있었습니다. 실용적이긴 하지만, 제곱합 오류 손실은 크고 작은 경계 상자의 오류에 동일하게 가중치를 부여했기 때문에 탐지 작업에 이상적으로 적합하지 않았습니다.

## 영향 및 중요성

YOLO의 도입은 객체 탐지 연구에서 근본적인 변화를 가져왔으며, 복잡한 다단계 파이프라인에서 통합된 종단 간 접근 방식으로 분야를 전환시켰습니다. 이 작업의 주요 중요성은 실시간 객체 탐지를 처음으로 실용화하여 자율 주행, 로봇 공학, 감시 및 즉각적인 시각적 이해가 필요한 대화형 시스템에 적용할 수 있게 했다는 점입니다.

탐지를 단일 회귀 문제로 다루는 개념적 단순성은 SSD, RetinaNet 및 이후 YOLO 버전(YOLOv2-v8)을 포함한 수많은 후속 작업에 영감을 주었습니다. 이는 실시간 탐지 애플리케이션을 계속 지배하는 "단일 샷" 탐지기 전체 계열을 탄생시켰습니다.

![YOLO Detection Examples](https://paper-assets.alphaxiv.org/figures/1506.02640v5/img-5.jpeg "YOLO Detection Examples")_그림 6: 다양한 이미지(자연 사진 및 예술 작품 포함)에 대한 YOLO 탐지 결과로, 시스템이 다양한 시각적 맥락에서 여러 객체 클래스를 동시에 탐지하는 능력을 보여줍니다._

속도 개선 외에도 YOLO는 객체 탐지에서 전역적 문맥 추론의 중요성을 입증했습니다. 고립된 영역이 아닌 전체 이미지를 처리함으로써, 총체적인 이해가 어떻게 오탐지(false positives)를 줄이고 도메인 변화에 대한 강건성(robustness)을 향상시킬 수 있는지를 보여주었습니다.

YOLO의 코드와 사전 학습된 모델의 오픈 소스 공개는 채택 및 추가 연구를 크게 가속화했으며, 이는 2010년대 가장 영향력 있는 컴퓨터 비전 논문 중 하나로 자리매김했습니다. 이후의 개선 사항들이 YOLO의 초기 한계점들 중 상당수를 해결했지만, 통합된 종단 간(end-to-end) 탐지라는 핵심 통찰은 현대 실시간 객체 탐지 시스템의 중심에 남아있습니다.

YOLO의 유산은 기술적 기여를 넘어, 근본적인 문제 공식을 재개념화하는 것이 성능과 실용성 모두에서 획기적인 발전을 가져올 수 있음을 보여줍니다. 이는 객체 탐지의 실제 배포를 제한했던 속도-정확도 상충 관계를 해결함으로써 학술 연구가 어떻게 현실 세계에 직접적인 영향을 미칠 수 있는지를 보여주는 좋은 예입니다.