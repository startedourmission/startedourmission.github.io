---
date: 2025-09-13
tags:
aliases:
image:
description: "LLM이 텍스트를 이해하는 첫 단계인 토큰화(Tokenization)의 다양한 방법을 정리합니다. 단어, 문자, 서브워드(BPE, WordPiece), SentencePiece 등 주요 토큰화 기법의 원리와 장단점을 비교하고, 모델 성능에 미치는 영향을 설명합니다."
---
딥러닝은 모든 것을 숫자로 처리합니다. NLP와 LLM도 예외는 아닙니다. 숫자만 다룰 줄 아는 LLM이 어떻게 수많은 종류의 언어를 구사할 수 있을까요? 그 비밀의 첫 번째 단추는 **토큰화(Tokenization)** 입니다. 지금부터 LLM이 우리의 언어를 어떻게 이해하고 처리하는지 알아봅니다.

## 토큰화란?

토큰화는 입력된 텍스트를 모델이 처리할 수 있는 작은 단위로 나누는 과정입니다. 마치 문장을 단어로 나누는 것처럼, 컴퓨터는 연속된 텍스트를 '토큰'이라는 의미있는 조각들로 분해합니다.

예를 들어, "안녕하세요"라는 문장은 다음과 같이 토큰화될 수 있습니다:

- 문자 단위: ["안", "녕", "하", "세", "요"]
- 형태소 단위: ["안녕", "하", "세요"]
- 서브워드 단위: ["안녕하", "세요"]

각각의 토큰은 고유한 숫자 ID로 매핑되어 모델의 입력으로 사용됩니다.

## 주요 토큰화 방법들

### 1. 단어 기반 토큰화 (Word-level Tokenization)

가장 직관적인 방법으로, 공백이나 구두점을 기준으로 단어를 분리합니다.

**예시:**

- "I love AI" → ["I", "love", "AI"]
- "안녕하세요, 반갑습니다!" → ["안녕하세요", ",", "반갑습니다", "!"]

**장점:**

- 직관적이고 해석하기 쉬움
- 단어 의미가 보존됨

**단점:**

- **희귀 단어(rare word) 문제**: 사전에 없는 단어는 `<UNK>` 토큰으로 처리
- **어휘 사전 크기 폭증**: 모든 단어형을 포함하려면 수백만 개의 토큰 필요
- **한국어/일본어 같은 교착어에 부적합**: 어미 변화로 인한 무한한 단어 형태

### 2. 문자 기반 토큰화 (Character-level Tokenization)

한 글자씩 토큰으로 분리하는 방법입니다.

**예시:**

- "AI" → ["A", "I"]
- "나는" → ["나", "는"]
- "Hello!" → ["H", "e", "l", "l", "o", "!"]

**장점:**

- 작은 어휘 사전 크기 (한글: ~11,000자, 영어: 26+특수문자)
- 사전에 없는 단어 문제 완전 해결
- 오타나 신조어에 강건함

**단점:**

- **시퀀스 길이 급격히 증가**: "tokenization" → 12개 토큰
- 학습 비효율성과 메모리 부담 증가
- 문자 간 관계 학습의 어려움

### 3. 서브워드 기반 토큰화 (Subword-level Tokenization)

현재 대부분의 LLM에서 사용되는 방법으로, 자주 등장하는 서브워드 조각 단위로 분리합니다.

#### BPE (Byte Pair Encoding)

가장 빈번하게 등장하는 문자 쌍을 반복적으로 병합하여 서브워드를 생성합니다.

**과정:**

```
1. 초기 상태: ["l", "o", "w", "e", "r", "e", "d"]
2. 병합 1: ["lo", "w", "e", "r", "e", "d"] (l+o가 가장 빈번)
3. 병합 2: ["lo", "we", "r", "e", "d"] (w+e가 가장 빈번)
4. 병합 3: ["low", "er", "e", "d"] (lo+w가 가장 빈번)
5. 최종: ["lower", "ed"]
```

**사용 모델**: GPT-2, GPT-3, RoBERTa 등

#### WordPiece (BERT 방식)

BPE와 유사하지만 확률 기반으로 병합을 결정합니다.

**예시:**

- "unbelievable" → ["un", "##believ", "##able"] (`##`는 이어짐 표시)
- "playing" → ["play", "##ing"]

**특징:**

- 언어 모델 확률을 고려한 병합
- `##` 접두사로 서브워드 구분

#### Unigram Language Model (SentencePiece 기본)

여러 분해 후보 중 확률적으로 가장 효율적인 분해를 선택합니다.

**예시:**

- "나는AI가좋아" → ["나", "는", "AI", "가", "좋", "아"]
- "인공지능" → ["인공", "지능"] 또는 ["인공지", "능"]

**특징:**

- 확률적 분해로 더 유연함
- 동일 입력도 다르게 토큰화될 수 있음

### 4. 바이트 기반 토큰화 (Byte-level Tokenization)

입력을 먼저 바이트(0~255) 단위로 변환한 후 BPE를 적용합니다.

**방식:**

1. 텍스트를 UTF-8 바이트로 변환
2. 바이트 시퀀스에 BPE 적용

**예시:**

- "🤖" (로봇 이모지) → [240, 159, 164, 150] → BPE 적용

**장점:**

- 어떤 언어든 표현 가능
- 이모지, 특수문자 문제없음
- 고정된 작은 기본 어휘 (256개 바이트)

**사용 모델**: GPT-2, GPT-3, Codex

### 5. 형태소 기반 토큰화 (Morpheme-level Tokenization)

언어학적 의미 단위인 형태소로 분리하는 방법입니다.

**예시:**

- "나는 밥을 먹었다"
    - 형태소 분석: ["나", "는", "밥", "을", "먹", "었", "다"]
    - 단어 분석: ["나는", "밥을", "먹었다"]

**장점:**

- 언어학적으로 의미있는 분해
- 한국어/일본어 같은 교착어에 효과적

**단점:**

- 언어별 별도 처리기 필요
- 모델의 범용성 저하
- 형태소 분석기의 정확도에 의존

### 6. SentencePiece (구글 개발)

언어에 독립적인 토큰화를 제공하는 통합 프레임워크입니다.

**특징:**

- **공백을 특수 문자(`_`)로 처리**: 띄어쓰기가 없는 언어도 처리 가능
- BPE 또는 Unigram LM 알고리즘 선택 가능
- 전처리 최소화: 원본 텍스트를 그대로 처리

**예시:**

- "Hello world" → ["▁Hello", "▁world"] (`▁`는 공백 표시)
- "안녕하세요" → ["▁안녕", "하세요"]

**사용 모델**: T5, LLaMA, PaLM, Qwen 등 최신 모델 다수

### 7. 토큰화 최소화 방식 (Byte/Character Streaming)

아예 토큰화를 최소화하고 바이트나 문자 단위로 직접 처리하는 접근법입니다.

**방식:**

- 입력을 바이트 스트림으로 처리
- 별도의 어휘 사전 없이 256개 바이트값만 사용

**장점:**

- 극도로 간단한 구조
- 사전 관리 불필요
- 언어/도메인 제약 없음

**단점:**

- 시퀀스 길이 대폭 증가
- 연산량과 메모리 사용량 증가

**연구 동향**: OpenAI의 일부 연구, Meta의 Character Tokenizer 등

## 토큰화 방법별 비교 요약

|방법|어휘 크기|시퀀스 길이|장점|단점|주요 사용 모델|
|---|---|---|---|---|---|
|**단어 기반**|매우 큰|짧음|직관적, 의미 보존|희귀어 문제, 사전 크기|초기 모델들|
|**문자 기반**|작음|매우 긺|범용성, 강건성|비효율적|일부 특수 용도|
|**BPE**|중간|중간|균형잡힌 성능|언어별 편향|GPT-2/3, RoBERTa|
|**WordPiece**|중간|중간|확률 기반 최적화|복잡한 전처리|BERT, DistilBERT|
|**Unigram**|중간|중간|유연한 분해|비결정적|T5, mT5|
|**바이트 기반**|작음|긺|완전한 범용성|긴 시퀀스|GPT-2/3, Codex|
|**SentencePiece**|중간|중간|언어 독립적|-|LLaMA, PaLM, Qwen|
|**Byte Stream**|최소|최대|극단적 단순함|매우 비효율적|실험적 연구|

현재 가장 널리 사용되는 방법은 **BPE, WordPiece, SentencePiece**이며, 특히 SentencePiece가 다국어 지원과 범용성 때문에 최신 LLM들의 표준이 되어가고 있습니다.

## 토큰화가 모델 성능에 미치는 영향

### 어휘 사전 크기와 표현력의 트레이드오프

토큰화 방식에 따라 어휘 사전의 크기가 결정되며, 이는 모델의 매개변수 수와 직접적으로 연관됩니다. 일반적으로 LLM들은 다음과 같은 어휘 사전 크기를 가집니다:

- GPT-3/4: 약 50,000개
- BERT: 약 30,000개
- T5: 약 32,000개

### 토큰 효율성

같은 의미를 표현하는데 필요한 토큰 수가 다르면 모델의 효율성에 영향을 미칩니다. 예를 들어:

```
"인공지능" → ["인공", "지능"] (2 토큰)
"AI" → ["AI"] (1 토큰)
```

영어에 최적화된 토크나이저를 사용할 경우, 한국어는 상대적으로 더 많은 토큰으로 분해되어 효율성이 떨어질 수 있습니다.

## 실제 예시로 보는 토큰화

다양한 문장들이 어떻게 토큰화되는지 살펴보겠습니다:

**한국어 예시:**

- "안녕하세요" → ["안녕", "하세요"]
- "머신러닝을 공부합니다" → ["머신", "러닝", "을", "공부", "합니다"]

**영어 예시:**

- "Hello world" → ["Hello", " world"]
- "tokenization" → ["token", "ization"]

**특수 문자:**

- "claude@anthropic.com" → ["claude", "@", "anth", "rop", "ic", ".", "com"]

## 토큰화의 한계와 해결 방안

### 1. 언어별 편향

대부분의 토크나이저가 영어 중심으로 설계되어 다른 언어의 토큰 효율성이 떨어집니다. 이를 해결하기 위해:

- 다국어 데이터셋으로 토크나이저 훈련
- 언어별 특화 토크나이저 개발
- 적응형 토큰화 기법 연구

### 2. 도메인 특화 문제

의료, 법률, 기술 분야의 전문 용어들이 비효율적으로 토큰화되는 문제가 있습니다. 이는 도메인 특화 토크나이저나 어휘 확장을 통해 해결할 수 있습니다.

### 3. 문맥 정보 손실

토큰 단위로 처리하다 보면 단어나 구문의 의미가 분절될 수 있습니다. 이는 모델의 어텐션 메커니즘과 위치 인코딩을 통해 어느 정도 보완됩니다.

## 미래의 토큰화 기술

### 동적 토큰화

입력 텍스트의 특성에 따라 토큰화 방식을 동적으로 조정하는 연구가 진행되고 있습니다.

### 의미 기반 토큰화

단순한 통계적 빈도가 아닌 의미적 유사성을 고려한 토큰화 방법들이 제안되고 있습니다.

### 토큰 없는 모델

아예 토큰화 과정을 생략하고 문자나 바이트 단위에서 직접 학습하는 모델들도 연구되고 있습니다.

## 결론

토큰화는 LLM의 성능을 좌우하는 중요한 요소입니다. 효율적인 토큰화는 모델의 표현력을 높이고 계산 비용을 줄이는 데 기여합니다. 특히 한국어와 같은 비영어권 언어를 다룰 때는 토큰화 방식의 선택이 더욱 중요해집니다. 