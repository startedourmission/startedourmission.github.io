---
aliases:
  - "공유는 배려다 : 집단적 RL 경험 공유를 통한 효율적인 LM 후훈련"
date: 2025-09-18
tags:
image: "![[SharingisCaring_1.png]]"
---
강화학습을 통한 언어모델 후훈련이 점점 중요해지고 있습니다. LLM 훈련에 필요한 비용은 천문학적이며 엄청난 병렬 처리 기술이 필요합니다. DeepSeek-R1-Zero처럼 지도학습 없이도 복잡한 추론 능력을 향상시킬 수 있다는 것을 입증한 경우도 있습니다만 대부분의 상위 모델은 수많은 컴퓨터를 동시에 가동하고 모든 자원을 훈련에 투입합니다. 그러던 어느 날 수천 명이 맥북으로 언어 모델 훈련에 참여합니다. 거대한 GPU 클러스터가 아닌 각자 집에서 자신의 컴퓨터로 AI 모델을 훈련한겁니다. 이것이 바로 Gensyn AI 팀이 제안한 SAPO(Swarm sAmpling Policy Optimization)의 정체입니다. 

>J. Amico, G. P. Andrade, J. Donaghy, B. Fielding, T. Forbus, H. Grieve, S. Kara, J. Kolehmainen, Y. Lou, C. Nies, E. P. F. Nuño, D. Ortega, S. Rastogi, A. Virts, and M. J. Wright, "Sharing is Caring: Efficient LM Post-Training with Collective RL Experience Sharing", arXiv preprint arXiv:2509.08721, 2024.

Gensyn는 LLM 모델 개발 회사는 아닙니다. 블록체인 기반 회사입니다. SAPO에 블록체인을 사용한 것도 아닙니다. 그들이 개발한 것은 기존 모델을 훈련할 수 있는 방법론과 플랫폼을 제공합니다. 신기한 비즈니스 모델이네요. 그런데 이 SAPO라는 것이 재밌습니다. 전통적인 중앙집중식 시스템과 다르게 각 노드가 자신만의 정책을 관리하면서 생성한 경험(rollout)을 다른 노드들과 공유하는 분산형 구조입니다. 이를테면 학생들이 서로 문제 풀이 과정을 공유하며 함께 공부하는 것 같습니다.

![[SharingisCaring_1.png]]

# 요약

SAPO는 이질적인 컴퓨팅 노드들로 구성된 분산 네트워크에서 동작하는 완전 분산형 비동기 강화학습 후훈련 알고리즘입니다. 핵심 아이디어는 각 노드가 독립적으로 훈련하면서도 생성한 rollout을 네트워크와 공유하여 집단 지능을 활용하는 것입니다.

**주요 기술적 특징:**

- **모델**: 정책 모델과 보상 모델을 각 노드가 독립적으로 관리
	- **통제 실험에서:**
		- **Qwen2.5 0.5B 파라미터**
		- 8개 인스턴스를 만들어서 스웜 구성
	
	- **대규모 데모에서:**
		- **Qwen2.5 0.5B**
		- **Qwen3 0.6B**
		- 커뮤니티 멤버들이 각자 선택한 다양한 모델들

- **데이터셋**: ReasoningGYM을 사용하여 9개 추론 태스크에서 검증
- **평가 지표**: 누적 보상으로 성능 측정, 기존 대비 최대 94% 향상
- **훈련 방법**: GRPO(Group Relative Policy Optimization) 기반으로 각 노드가 독립 업데이트

# 논문 상세

## 1. 서론과 동기

기존 강화학습 후훈련은 대규모 GPU 클러스터에서 정책 가중치를 동기화하며 진행됩니다. 이는 효과적이지만 비용이 많이 들고, 통신 병목현상이 발생하며, 신중하게 설계된 인프라가 필요합니다.

SAPO의 핵심 통찰은 간단합니다. 각 노드가 생성한 rollout을 일반 텍스트 형태로 공유하면, 모델 아키텍처나 하드웨어에 관계없이 다른 노드들이 이를 활용할 수 있다는 것입니다. 이렇게 하면 동기화 오버헤드 없이 이질적인 참여자들이 협업할 수 있습니다.

## 2. 방법론

### 2.1 스웜(Swarm) 구조

SAPO의 네트워크는 $N$개의 노드로 구성되며, 각 노드 $n$은 다음을 가집니다:

- 질문 집합 $Q^n$과 각 질문에 대한 정답 $y_q$
- 데이터셋 $D^n := {(q, y_q) | q \in Q^n}$
- 정책 $\pi^n$ (언어모델)

각 노드는 질문 $q$에 대해 $L^n$개의 답안을 생성하여 rollout $R^n(q) := {a^n_1(q), ..., a^n_{L^n}(q)}$를 만듭니다.

### 2.2 SAPO 알고리즘

SAPO의 핵심 과정은 다음과 같습니다:

1. **롤아웃 생성**: 각 노드가 배치 질문 $B^n \subseteq Q^n$에 대해 답안 생성
2. **경험 공유**: 노드 $n$이 선택된 질문들에 대해 다음을 방송 $$C^n(q) := (q, y_q, R^n(q), M^n)$$
3. **훈련 셋 구성**: 로컬 $I^n$개와 외부 $J^n$개 샘플을 조합 $$T^n = \text{로컬 샘플} \cup \text{외부 샘플}$$
4. **정책 업데이트**: 보상 모델 $\rho^n$으로 보상 계산 후 정책 경사 알고리즘으로 업데이트

중요한 점은 각 노드가 자신의 필터링 전략을 완전히 제어할 수 있다는 것입니다. 예를 들어, 0 advantage를 가진 rollout은 버리고 나머지에서 균등하게 샘플링할 수 있습니다.

## 3. 실험 설계

### 3.1 통제 실험

8개의 Qwen2.5 0.5B 파라미터 모델로 스웜을 구성했습니다. ReasoningGYM 데이터셋에서 9개 태스크를 사용했습니다:

- **base_conversion**: 진법 변환
- **basic_arithmetic**: 기초 산술
- **arc_1d**: 1차원 추상 추론
- **bf**: 알고리즘 추론
- **propositional_logic**: 명제논리
- **fraction_simplification**: 분수 기약분
- **decimal_arithmetic**: 소수 연산
- **calendar_arithmetic**: 달력 문제
- **binary_matrix**: 이진 행렬 추론

각 노드는 라운드마다 8개 질문을 받고, 질문당 8개 완성을 생성합니다. GRPO로 정책을 업데이트하며, KL 발산 패널티는 0으로 설정했습니다.

### 3.2 대규모 스웜 데모

수천 명의 Gensyn 커뮤니티 멤버들이 참여한 오픈소스 데모도 진행했습니다. 다양한 하드웨어(MacBook 등)에서 실행되는 이질적인 모델들이 집단적으로 훈련하는 환경에서 테스트했습니다.

## 4. 실험 결과

### 4.1 통제 실험 결과

4가지 구성을 테스트했습니다:

- 8 로컬 / 0 외부 (기준선)
- 6 로컬 / 2 외부
- 4 로컬 / 4 외부
- 2 로컬 / 6 외부

**핵심 발견:**

- 4 로컬 / 4 외부 구성이 최고 성능: 기준선 대비 **94% 향상**
- 누적 보상: 4/4 (1093.31) > 2/6 (945.87) > 6/2 (854.43) > 8/0 (561.79)
- 외부 rollout 의존도가 너무 높으면 오히려 불안정해짐

흥미로운 점은 "Aha moment" 현상입니다. 한 에이전트가 좋은 해법을 찾으면 이것이 스웜 전체로 퍼져 집단 성능을 향상시킵니다.

### 4.2 균형의 중요성

4 로컬 / 4 외부가 최적인 이유:

1. **너무 적은 외부 의존 (6/2)**: 스웜 효과를 충분히 활용하지 못함
2. **너무 높은 외부 의존 (2/6)**: 두 가지 네트워크 효과로 불안정
    - 고성능 에이전트가 저성능 에이전트 답안에 악영향
    - 에이전트들이 기여는 적게 하고 의존만 많이 하면 공유 풀 품질 저하

### 4.3 대규모 데모 결과

- Qwen2.5 0.5B 모델: 스웜 참여 시 지속적 성능 향상
- 약 175 정규화 라운드 후 통계적으로 유의미한 개선 확인
- 더 강한 모델(Qwen3 0.6B)은 스웜 안팎에서 비슷한 성능
- 중간 용량 모델이 SAPO 혜택을 가장 크게 받음

## 5. 미래 방향

### 5.1 이질성 확대

더 다양한 기반 모델이나 전문화된 태스크 환경에서의 테스트가 필요합니다. 심지어 적절한 인센티브 구조가 갖춰진다면 인간 같은 비전통적 정책도 스웜에 참여할 수 있을 것입니다.

### 5.2 안정성 개선

외부 rollout에 과도하게 의존할 때 발생하는 진동과 망각 현상을 해결해야 합니다. 보상 기반 공유, RLHF, 또는 생성적 검증자와의 하이브리드 접근이 도움이 될 수 있습니다.

### 5.3 다중 모달리티

SAPO는 데이터 모달리티에 구애받지 않습니다. 이미지 기반 스웜에서는 각 노드가 자신만의 "미적 감각"을 보상 메커니즘으로 정의하고, 이것이 다른 노드들에게 "좋다"고 평가받으면 스웜 전체의 이미지 스타일에 영향을 줄 수 있습니다.

## 결론

SAPO는 강화학습 후훈련의 패러다임을 바꿀지도 모릅니다. 중앙집중식 시스템의 병목현상과 높은 비용은 LLM 발전의 천장이죠. SAPO의 집단 지성이 개별 모델의 한계를 뛰어넘을 수 있을지 지켜보면 재미있을 것 같습니다. 단순한 경험 공유만으로도 94%의 성능 향상을 달성했다면 대단한 일입니다. 큰 조직이 아니어도 고성능 모델을 개발할 수 있다는 뜻입니다. "공유는 배려"라는 제목처럼, 각자의 경험을 나누는 것만으로도 모두가 더 나은 결과를 얻을 수 있다는 것을 보여준 흥미로운 연구입니다.