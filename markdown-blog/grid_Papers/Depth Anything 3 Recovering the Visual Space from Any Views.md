---
date: 2025-11-18
tags:
  - 논문
  - 영상처리
  - Headliner
aliases:
  - "Depth Anything 3: Recovering the Visual Space from Any Views"
image: "![[1-DepthAnything3.png]]"
description: 또이트댄스입니다. Depth Anything 3는 한 장의 이미지든 여러 장의 영상이든, 카메라 포즈 정보가 있든 없든 상관없이 3D 기하 정보를 예측하는 모델입니다. 평범한 트랜스포머 하나와 단순한 깊이-광선(depth-ray) 표현으로 이전 최고 성능을 44% 능가하는 성능을 달성했으며, 모든 데이터를 공개 학술 데이터셋으로만 학습했습니다.
---
Depth Anything 3는 한 장의 이미지든 여러 장의 영상이든, 카메라 포즈 정보가 있든 없든 상관없이 3D 기하 정보를 예측하는 모델입니다. 평범한 트랜스포머 하나와 단순한 깊이-광선(depth-ray) 표현으로 이전 최고 성능을 44% 능가하는 성능을 달성했으며, 모든 데이터를 공개 학술 데이터셋으로만 학습했습니다.

## 논문 정보

**제목**: Depth Anything 3: Recovering the Visual Space from Any Views

**저자**: Haotong Lin, Sili Chen, Jun Hao Liew, Donny Y. Chen, Zhenyu Li, Guang Shi, Jiashi Feng, Bingyi Kang (ByteDance)

**발표**: 2025년 11월 13일 (arXiv:2511.10647)

> H. Lin, S. Chen, J. H. Liew, D. Y. Chen, Z. Li, G. Shi, J. Feng, and B. Kang, "Depth Anything 3: Recovering the visual space from any views," arXiv preprint arXiv:2511.10647, 2025.


![[1-DepthAnything3.png]]

---

## 요약

|항목|내용|
|---|---|
|**핵심 기여**|단일 트랜스포머로 임의의 뷰 개수 처리, 최소한의 예측 목표(깊이+광선)로 통합된 3D 기하 추정|
|**모델 구조**|사전학습된 DINOv2 + 입력 적응형 크로스뷰 어텐션 + 이중 DPT 헤드|
|**훈련 방식**|교사-학생 패러다임: 합성 데이터로 교사 모델 학습 → 실제 데이터에 의사 라벨 생성|
|**데이터셋**|공개 학술 데이터셋만 사용(총 27개 데이터셋, 70만 장 이상)|
|**평가 지표**|카메라 포즈 정확도, 기하 재구성 정확도, 새로운 뷰 합성 품질|
|**주요 성과**|VGGT 대비 포즈 정확도 44.3% 향상, 기하 정확도 25.1% 향상|

---

## 상세 분석

### 1. 서론

3D 시각 작업들은 개념적으로 겹치지만 별개의 모델로 다루어져 왔습니다. 단안 깊이 추정, 구조 복원(SfM), 다중뷰 스테레오(MVS), SLAM 등으로 나뉩니다. 각각 고도로 최적화된 아키텍처를 필요로 했지만, 근본적으로는 같은 문제를 푸는 것입니다. 임의의 시각 입력으로부터 3D 구조를 복원하는 것이죠.

지난 몇 년간 딥러닝 기반의 시도들이 있었지만 대부분 세 가지 한계를 가졌습니다.

첫째, **복잡한 아키텍처**입니다. VGGT 같은 이전 최고 모델들은 여러 개의 트랜스포머를 쌓거나 특별한 모듈을 추가했습니다. 이는 설계의 자유도를 높이지만, 동시에 예측이 어떻게 만들어지는지 더 불명확하게 만듭니다.

둘째, **중복된 예측 목표**입니다. 포즈, 점 맵, 깊이를 동시에 예측하거나, 각 과제별로 별도 손실 함수를 사용하는 식의 다중 작업 학습은 모델을 혼란스럽게 만들 수 있습니다.

셋째, **사전학습의 활용 미흡**입니다. 새로운 모델들이 처음부터 학습되면서 대규모 시각 모델의 강력한 특징 추출 능력을 제대로 활용하지 못했습니다.

ByteDance 팀이 제시한 Depth Anything 3는 이 모든 문제에 정면으로 도전합니다. "최소 모델링"이라는 철학 아래, 단 하나의 평범한 트랜스포머로도 충분한지, 그리고 정말 필요한 예측 목표가 무엇인지를 묻습니다.

### 2. 기존 연구

컴퓨터 비전의 기하 추정 분야는 전통적 방법에서 학습 기반 방법을 거쳐 기초 모델로 진화해왔습니다. 각 단계마다 일반화와 속도 면에서 진전이 있었습니다.

**전통적 3D 비전의 한계**

전통적 접근은 특징 검출, 매칭, 포즈 추정, 다중뷰 스테레오로 나뉩니다. 잘 짜인 파이프라인이지만 취약한 부분이 있었습니다. 텍스처가 적거나 반사가 많거나, 큰 시점 변화가 있을 때 기하학적 일관성이 무너집니다.

**학습 기반의 진화**

2010년대 후반, DUSt3R과 같은 변환기 기반 모델들이 등장했습니다. 이들은 두 이미지 사이의 점 맵을 직접 예측해서 포즈와 깊이를 계산했습니다. 순수 피드포워드 방식으로 모듈 파이프라인의 복잡성을 줄였죠.

이후 VGGT, Pi3, Fast3R 같은 모델들이 이를 확장했습니다. 임의의 뷰 개수를 처리하고, 더 나은 기하를 예측했습니다. 하지만 더 많은 뷰를 처리할수록, 더 복잡한 아키텍처가 필요합니다.

**DA3의 차별점**

DA3는 복잡함 대신 단순함을 선택합니다. 

- 사전학습된 트랜스포머 하나면 충분하다
- 깊이와 광선 맵만으로 기하와 포즈를 모두 복원할 수 있다

더 적은 것으로 더 많은 것을 한다는 철학! 이를테면 인지적 미니멀리즘입니다.

### 3. 깊이-광선 표현

DA3는 전통적인 포즈 행렬이나 점 맵 대신 픽셀 정렬 광선 맵을 사용합니다.

**왜 광선인가?**

일반적으로 3D 점은 다음과 같이 계산됩니다:

$$P = R(D(u,v) \cdot K^{-1}p) + t$$

여기서 $R$은 회전 행렬, $D$는 깊이, $K$는 내부 매개변수, $t$는 변환입니다.

문제는 $R$을 직접 예측하기 어렵다는 것입니다. 정규직교(orthogonal) 제약이 있기 때문입니다.

DA3는 대신 광선으로 표현합니다. 각 픽셀 $p$에 대해:

$$r = (t, d)$$

여기서 $t$는 광선의 원점(카메라 중심), $d$는 광선의 방향입니다. $d$를 정규화하지 않으므로 투영 스케일을 보존합니다.

그러면 3D 점은:

$$P = t + D(u,v) \cdot d$$

이렇게 하면 매우 깔끔합니다. 광선 맵은 단순히 픽셀별 벡터 6개(원점 3개 + 방향 3개)일 뿐입니다.

**포즈 복원**

역함수도 간단합니다. 광선 맵에서 카메라 포즈를 계산하려면:

1. **카메라 중심**: 모든 픽셀의 원점을 평균

$$t_c = \frac{1}{H \times W} \sum_{h,w} M(h,w,:3)$$

2. **회전과 내부 매개변수**: 호모그래피로 계산

표준 "정체성" 카메라에서 대상 카메라로의 변환을 구하면, 이는 호모그래피 $H = KR$입니다. DLT(Direct Linear Transform) 알고리즘으로 최소제곱법 해를 구하고, RQ 분해로 $K$와 $R$을 분해합니다.

이 방식의 아름다움은 **최소성**입니다. 깊이와 광선 두 가지만으로 기하의 모든 정보를 담을 수 있고, 포즈 복원이 계산 가능하면서도 의미 있습니다.

### 4. 아키텍처

DA3의 백본은 특별하지 않습니다. DINOv2 같은 사전학습된 비전 트랜스포머입니다. 특별한 부분은 여러 뷰를 효율적으로 처리하는 방식입니다.

**입력 적응형 크로스뷰 어텐션**

$N$개의 이미지가 입력되면 각 이미지는 패치 토큰으로 변환됩니다. 문제는 이들이 어떻게 상호작용하는가입니다.

DA3의 해결책은 토큰 재정렬입니다.

- **초기 $L_s$ 레이어**: 각 이미지 내 자기 어텐션(within-view)
- **나머지 $L_g$ 레이어**: 크로스뷰/윗-뷰 어텐션 교대

예를 들어, 3개 이미지의 토큰을 [이미지1, 이미지2, 이미지3] 순서로 놓고 전체 어텐션을 계산하면 자연스럽게 뷰 간 정보 교환이 일어납니다.

실험에서 비율을 $L_s : L_g = 2:1$로 설정했을 때 최적입니다. 모든 뷰가 균형 있게 기여하는 지점이죠.

**카메라 토큰 주입**

포즈 정보가 있으면 이를 활용해야 합니다. DA3는 각 이미지 앞에 카메라 토큰을 추가합니다:

- **포즈 있음**: MLP로 인코딩된 카메라 매개변수 (초점 거리, 쿼터니언, 변환)
- **포즈 없음**: 학습 가능한 공유 토큰

이 토큰들이 모든 어텐션 레이어에 참여합니다. 선택사항이지만 영향력이 있습니다.

**이중 DPT 헤드**

마지막 단계에서 깊이 맵과 광선 맵을 모두 예측해야 합니다. 일반적으로 별도 헤드를 사용했지만, DA3는 공유 재조립 모듈을 통과한 후 두 개의 서로 다른 융합 레이어 세트로 분기합니다.

이 설계는 두 작업이 같은 특징 공간에서 출발하지만, 마지막 단계에서 분화하도록 합니다. 상호작용을 유지하면서 중복을 피하는 균형입니다.

### 5. 교사-학생 훈련

실제 데이터는 노이즈가 많고 불완전합니다. DA3는 합성 데이터로 훈련한 교사 모델로부터 고품질 의사 라벨을 생성해 이 문제를 해결합니다.

**문제 상황**

실제 카메라(LiDAR, RGB-D)로 수집한 깊이는 여러 문제를 가집니다 (Figure 4 참조):

- 스파스한 포인트 (특히 변두리)
- 노이즈 (센서의 불확실성)
- 누락된 영역 (반사, 투명 물질)
- 배경 오류 (동적 객체, 반사 표면)

COLMAP이나 다른 3D 재구성 방법도 마찬가지입니다. 특히 저텍스처 영역에서 실패합니다.

**교사 모델 구성**

합성 데이터만으로 훈련한 모노큘러 깊이 추정 모델입니다:

- **데이터**: 20개 이상의 합성 데이터셋 (Hypersim, TartanAir, vKITTI2, Objaverse 등)
- **특징**: 실외, 실내, 객체 중심, 다양한 스타일의 장면
- **출력**: 스케일-시프트 불변 지수 깊이 (선형 대신 지수를 사용하여 근처 물체의 미세함 증가)

손실 함수에는 표준 항목 외에도 거리 가중 표면 법선 손실이 포함됩니다:

$$L_N = E(\hat{n}_m, n_m) + \sum_{i=0}^{4} E(\hat{n}_i, n_i)$$

인접한 픽셀들의 법선이 중심 픽셀과 얼마나 일치하는지 측정합니다. 이는 로컬 기하를 세밀하게 조정합니다.

**의사 라벨 정렬**

교사 모델의 상대 깊이 $\tilde{D}$를 실제 깊이 $D$에 정렬합니다:

$$(s, t) = \arg\min_{s>0,t} \sum_p m_p (s\tilde{D}_p + t - D_p)^2$$

RANSAC 최소제곱법으로 스케일과 시프트를 추정합니다. 이렇게 정렬된 라벨은 스케일-일관성을 유지하면서 세밀한 세부 정보를 추가합니다.

**학생 모델 훈련**

DA3는 이 의사 라벨로 훈련됩니다:

- 128개 H100 GPU에서 200k 스텝
- 처음 120k 스텝: 실제 깊이 사용
- 나머지 80k 스텝: 교사 라벨로 전환
- 해상도: 504 × 504부터 가변 해상도까지
- 뷰 개수: 2~18개 균등 샘플링

이 점진적 전환은 실제 포즈-기하 일관성을 먼저 학습한 후, 세부 정보로 다듬는 것입니다.

### 6. 새로운 벤치마크

기존 벤치마크는 깊이만 또는 포즈만 평가했습니다. DA3 팀은 "시각 기하" 벤치마크를 만들어 포즈와 기하의 일관성을 함께 평가합니다.

**벤치마크 파이프라인**

1. **포즈 추정 정확도**: 이미지 쌍 간 상대 회전/변환 오차
2. **기하 재구성 정확도**: 예측 포즈 + 예측 깊이로 포인트 클라우드 재구성 → 지상진 클라우드와 비교
3. **시각 렌더링**: 새로운 뷰 합성 품질 (PSNR, SSIM, LPIPS)

**포즈 메트릭**

두 이미지 간 상대 포즈 오차를 평가합니다:

- **RRA (Relative Rotation Accuracy)**: 회전 각도 오차
- **RTA (Relative Translation Accuracy)**: 변환 오차
- **AUC**: 오차 임계값에 따른 정확도의 곡선 아래 면적

보고된 임계값: 3도 vs 30도

**기하 메트릭**

Chamfer Distance의 변형:

- **정확도 (Accuracy)**: 재구성 점이 지상진에 가까운 정도 ($\text{dist}(R \to G)$)
- **완성도 (Completeness)**: 지상진 점이 재구성에 있는 정도 ($\text{dist}(G \to R)$)
- **F1-점수**: 두 지표의 조화평균

임계값 $d$를 기준으로 정밀도와 재현율을 계산합니다.

**데이터셋 (89개 장면)**

- **HiRoom** (30 신실): 합성, Blender, 전문 아티스트 제작
- **ETH3D** (11 장면): 실내/외부, 레이저 센서, 고해상도
- **DTU** (22 객체): 제어된 조건, 49개 뷰
- **7Scenes** (7 장면): 저해상도, 모션 블러
- **ScanNet++** (20 장면): 실내, iPhone LiDAR, 고해상도

### 7. 결과

DA3는 SOTA 모델들을 상당한 여유로 능가합니다. 특히 흥미로운 점은 작은 버전도 훨씬 큰 이전 모델을 능가한다는 것입니다.

**포즈 정확도**

Table 2 결과:

|모델|매개변수|HiRoom (AUC3)|ETH3D (AUC3)|ScanNet++ (AUC3)|
|---|---|---|---|---|
|DUSt3R|0.57B|17.6|4.30|8.10|
|VGGT|1.19B|49.1|26.3|62.6|
|**DA3-Large**|**0.36B**|**58.7**|**32.2**|**60.2**|
|**DA3-Giant**|**1.10B**|**80.3**|**48.4**|**85.0**|

DA3-Large는 VGGT의 1/3 크기이면서도 더 나은 성능입니다. DA3-Giant는 HiRoom에서 80.3 vs VGGT의 49.1 (63% 향상).

**기하 재구성 정확도**

Table 3 결과 (F1-점수, DTU는 Chamfer Distance):

- HiRoom: DA3-Giant 85.1 vs VGGT 56.7
- ETH3D: DA3-Giant 79.0 vs VGGT 57.2
- DTU: DA3-Giant 1.85mm vs VGGT 2.05mm

**모노큘러 깊이**

Table 4 - 표준 벤치마크에서도 경쟁력:

|모델|KITTI|NYU|ETH3D|평가 순위|
|---|---|---|---|---|
|DA2|94.6|97.9|86.5|2.60|
|**DA3**|**95.3**|**97.4**|**98.6**|**2.20**|
|**Teacher**|**97.2**|**97.9**|**99.8**|**1.00**|

교사 모델이 최고 수준입니다. 학생 모델도 경쟁력 있습니다.

**새로운 뷰 합성**

Table 5 - 3DGS 기반 렌더링:

|모델|출처|DL3DV (PSNR)|T&T (PSNR)|MegaDepth (PSNR)|
|---|---|---|---|---|
|pixelSplat|특화|16.55|13.81|13.87|
|VGGT|다용도|20.96|17.18|16.45|
|**DA3**|다용도|**21.33**|**18.10**|**17.89**|

기하 재구성 능력이 NVS 성능과 직결됨을 보여줍니다.

### 8. 한계와 확장성

DA3는 강하지만 현실적 한계가 있습니다. 계산량, 동적 장면, 약한 기하학적 단서가 있는 환경에서의 어려움을 이해하는 것이 중요합니다.

**계산량과 확장성**

- DA3-Giant: 128 H100 GPU, 약 10일
- 한 장면에서 최대 900~1000개 이미지 처리 가능 (A100 80GB)
- 속도: A100에서 이미지당 약 37.6 FPS (32개 이미지 기준)

이는 학술적으로는 합리적이지만, 실시간 애플리케이션(로봇, AR)에는 제약이 있습니다.

**동적 장면**

DA3는 정적 장면을 가정합니다. 사람이 움직이거나 객체가 변형되는 상황에서는 성능이 저하됩니다. 기하 일관성 가정이 깨지기 때문입니다.

**저텍스처 영역**

비록 교사 모델이 도움이 되지만, 본질적으로 특징이 없는 영역(하얀 벽, 물 표면)에서는 여전히 어렵습니다.

**포즈 초기화**

포즈가 너무 크게 잘못 초기화되면 복구 어렵습니다. 기하 재구성은 포즈에 매우 민감합니다.

**데이터 편향**

모든 훈련 데이터가 공개 학술 데이터셋이므로, 특정 스타일이나 조건에 편향될 수 있습니다. 실제 센서 특성(예: 특정 깊이 센서의 체계적 오류)을 반영하지 않을 수 있습니다.

**설계 선택의 의도**

몇 가지 설계 선택은 타협입니다:

1. **카메라 헤드의 필요성**: 광선 맵으로 포즈를 복구할 수 있지만, 계산 비용이 높아서 별도 경량 헤드를 추가했습니다. 이는 순수한 "최소 모델링" 철학에서는 벗어납니다.
    
2. **깊이 정규화**: 모든 손실을 정규화하기 위해 스케일을 사용합니다. 이는 안정성을 위해 필요하지만, 절대 스케일 정보를 어떻게 다루는지 명확하지 않습니다.
    
3. **가변 해상도 훈련**: 안정성을 위해 배치 크기를 동적으로 조정합니다. 이는 실제 추론에서 재현 가능한가?
    

---

## 결론

Depth Anything 3는 **단순성은 강력하다**고 이야기합니다.

**사전학습 단계**: 거대한 합성 데이터셋(20개 이상)으로 교사 모델을 훈련하여 고품질 깊이 예측 능력을 확보합니다. 이 모델은 특별한 아키텍처 없이 순수 DINOv2 + DPT로 구성됩니다.

**사후학습 단계**: 교사가 생성한 의사 라벨로 학생 모델(DA3)을 훈련합니다. 단 하나의 사전학습된 트랜스포머에 크로스뷰 어텐션 메커니즘과 깊이-광선 표현만 더하면, 임의의 뷰 개수에서 일관된 기하를 예측합니다.

**긍정적 평가**:

- **효율성**: 매개변수 1/3인 DA3-Large가 SOTA 능가
- **일반화**: 실내/외부, 객체 중심, 저/고해상도 모두 우수
- **확장성**: 2개 이미지부터 1000개 이상도 처리
- **투명성**: 최소한의 설계로 해석 가능성 증대

**현실적 한계**:

- 여전히 128 H100으로 10일 훈련 필요 (일반인 접근성 낮음)
- 동적 장면 미지원
- 저텍스처 영역 여전히 약함
- 절대 스케일 정보 처리 미흡

저자들은 다음을 제시합니다:

1. **동적 장면**: 시간 차원을 추가해 움직이는 객체 처리
2. **언어와 상호작용**: 자연어 지시로 특정 영역의 기하 개선
3. **대규모 사전학습**: 더 큰 모델과 데이터로 성능 향상

Depth Anything 3는 정교하면서도 깔끔하네요. 정말 필요한 것에 집중합니다.

이 접근법이 3D 비전 연구에 주는 교훈은:

- 사전학습된 모델의 활용 극대화
- 예측 목표의 최소화 (그러나 완전성 유지)
- 해석 가능한 표현의 선택

기하 추정, 기초 모델, 또는 단순 설계에 관심 있는 연구자라면 꼭 읽을 만한 논문입니다.