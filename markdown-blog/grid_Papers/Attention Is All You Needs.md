---
date: 2025-09-01
tags:
  - 논문
  - 딥러닝
  - Seq2Seq
aliases:
  - Transformer
image: "![[transformer_1.png]]"
---
이 논문이 소개하는 트랜스포머 아키텍처는 시퀀스-투-시퀀스 모델링의 한계를 해결합니다. 원래 이 분야는 순환 신경망(RNN), LSTM, GRU 등이 유명합니다. 모두 토큰별로 시퀀스를 순차적으로 처리하는 구조입니다. 이런 RNN 기반 모델은 기계 번역에서 강력한 성능을 보여주었지만 확장성과 훈련 효율성을 제한하는 내재적인 계산 병목 현상으로 어려움을 겪었습니다.

>A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention Is All You Need," presented at the 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA, 2017.


![[transformer_1.png]]

RNN은 순차적 특성 때문에 훈련 예제 내에서 병렬화가 불가능했습니다. 각 단계는 이전 단계의 은닉 상태를 입력으로 받기 때문입니다. 그러니까 RNN에 문장을 넣으면 앞에서부터 처리합니다. 앞 단계 계산이 끝나야 다음 단계 계산으로 넘어가는겁니다. 이것이 긴 시퀀스의 경우 상당한 계산 비효율성을 초래합니다. 또한 RNN은 소실 경사 문제와 정보가 네트워크를 통해 통과하는 긴 경로로 인해 장거리 의존성을 학습하는 데 어려움을 겪었습니다.

어텐션 메커니즘은 원래 있었습니다. RNN과 잘 결합하니 성능이 올라갑니다. 근데 지금까지는 어텐션이 주요 아키텍처 요소가 아닌 보조 구성 요소로 사용한 겁니다. 이 논문은 재귀(recurrence)와 컨볼루션(convolution)을 완전히 제거하고 **오직** 어텐션 메커니즘에만 의존하는 모델을 제안해 근본부터 바꿉니다. 이게 바로 **트랜스포머**입니다.

## 핵심 아키텍처 : 어텐션 메커니즘

트랜스포머는 인코더-디코더 프레임워크를 유지하지만 접근이 조금 다릅니다. 인코더와 디코더 모두 동일한 레이어 스택으로 구성됩니다. 인코더와 디코더는 6개의 레이어를 사용합니다. 각 레이어는 멀티 헤드 어텐션 메커니즘과 위치별 피드포워드 네트워크를 포함하며, 잔여 연결(residual connections)과 레이어 정규화(layer normalization)를 통해 연결됩니다.

트랜스포머의 기반은 어텐션 메커니즘, 특히 **스케일드 닷-프로덕트 어텐션**에 있습니다. 이 메커니즘은 쿼리(Q)와 키(K)의 내적을 취하고, 키 차원의 제곱근으로 스케일링한 다음 소프트맥스 함수를 적용하여 어텐션 가중치를 계산합니다.

$$
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^{T}}{\sqrt{d_k}} \right) V
$$

스케일링 인자 $dkdk$​​는 내적이 너무 커지는 것을 방지하여 소프트맥스 함수가 극도로 작은 기울기를 갖는 영역으로 밀려나는 것을 막아 학습을 방해하지 않도록 합니다.

![Scaled Dot-Product Attention](https://paper-assets.alphaxiv.org/figures/1706.03762v7/ModalNet-19.png "Scaled Dot-Product Attention")


_그림 2: 쿼리, 키, 값에서 행렬 곱셈, 스케일링, 마스킹(선택 사항), 소프트맥스, 최종 행렬 곱셈을 거쳐 흐름을 보여주는 스케일드 닷-프로덕트 어텐션 메커니즘._

이 기본적인 어텐션 함수를 기반으로 트랜스포머는 여러 어텐션 함수를 병렬로 수행하는 멀티 헤드 어텐션을 도입합니다. $d_{\text{model}}$​ 차원의 키, 값, 쿼리를 사용하는 단일 어텐션 함수 대신, 멀티 헤드 어텐션은 이러한 입력을 hh번 다른 학습된 선형 투영(linear projections)을 통해 더 낮은 차원의 공간으로 투영합니다. 그런 다음 각 투영된 버전에 어텐션 함수를 병렬로 적용하고, 결과는 연결(concatenated)되어 다시 투영됩니다.
$$
\begin{aligned}
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O \\
\text{where } \quad \text{head}_i &= \text{Attention}(Q W_i^Q, \; K W_i^K, \; V W_i^V)
\end{aligned}
$$

![Multi-Head Attention](https://paper-assets.alphaxiv.org/figures/1706.03762v7/ModalNet-20.png "Multi-Head Attention")_그림 3: 쿼리, 키, 값이 여러 번 선형 투영된 후 스케일드 닷-프로덕트 어텐션을 병렬로 적용하는 방식을 보여주는 멀티 헤드 어텐션 메커니즘._

이 다중 헤드 접근 방식은 모델이 서로 다른 표현 부분 공간의 정보를 서로 다른 위치에서 처리하여 데이터 내의 다양한 유형의 관계를 포착할 수 있도록 합니다.

## 위치 인코딩 및 모델 구성 요소

트랜스포머는 반복 또는 합성곱이 없기 때문에 시퀀스 순서에 대한 내재적인 이해가 없습니다. 이를 해결하기 위해 모델은 입력 임베딩에 추가되는 위치 인코딩을 통합합니다. 저자들은 서로 다른 주파수의 사인 함수를 사용합니다.

$$
PE(pos,2i)=sin⁡(pos/100002i/dmodel)PE(pos,2i)​=sin(pos/100002i/dmodel​)
$$
$$
PE(pos,2i+1)=cos⁡(pos/100002i/dmodel)PE(pos,2i+1)​=cos(pos/100002i/dmodel​)
$$

여기서 $pos$는 위치이고 $i$는 차원입니다. 이 접근 방식은 각 위치에 대한 고유한 인코딩을 제공하며, 모델이 훈련 중 경험한 시퀀스 길이보다 더 긴 시퀀스 길이로 외삽할 수 있도록 합니다.

인코더와 디코더의 각 계층에는 위치별 피드포워드 네트워크가 포함되어 있으며, 이는 ReLU 활성화와 함께 두 가지 선형 변환을 적용합니다.
$$
FFN(x)=max⁡(0,xW1+b1)W2+b2FFN(x)=max(0,xW1​+b1​)W2​+b2​
$$
이 네트워크는 각 위치를 독립적으로 그리고 동일하게 처리하여 어텐션 메커니즘 후에 모델에 비선형성을 추가합니다.

트랜스포머는 세 가지 다른 유형의 어텐션을 사용합니다.

1. **인코더 셀프 어텐션**: 각 위치는 입력 시퀀스의 모든 위치에 집중합니다.
2. **디코더 셀프 어텐션 (마스킹됨)**: 각 위치는 출력 시퀀스의 이전 위치에만 집중합니다.
3. **인코더-디코더 어텐션**: 디코더 위치는 모든 인코더 위치에 집중합니다.

## 훈련 전략 및 구현

모델은 표준 기계 번역 데이터셋인 WMT 2014 영어-독일어(450만 문장 쌍) 및 영어-프랑스어(3600만 문장)로 훈련되었습니다. 훈련에는 8개의 NVIDIA P100 GPU가 사용되었으며, 처음 4,000단계 동안 선형적으로 증가한 다음 단계 수의 역제곱근에 비례하여 감소하는 신중하게 설계된 학습률 스케줄이 활용되었습니다.

저자들은 두 가지 모델 변형을 구현했습니다. model​=512와 6개 레이어를 가진 기본 모델과 dmodel=1024dmodel​=1024와 전반적으로 확장된 차원을 가진 더 큰 모델입니다. 두 모델 모두 8개의 어텐션 헤드를 사용했으며, 일반화를 개선하기 위해 레이블 스무딩과 함께 정규화를 위한 드롭아웃을 통합했습니다.

## 실험 결과 및 성능

트랜스포머는 두 번역 벤치마크 모두에서 최첨단 결과를 달성했으며, 대규모 모델은 영어-독일어 번역에서 28.4 BLEU, 영어-프랑스어 번역에서 41.8 BLEU를 기록했습니다. 이 점수는 여러 시스템을 결합한 앙상블 모델을 포함하여 이전 최고 결과에 비해 상당한 개선을 나타냈습니다.

아마도 똑같이 중요했던 것은 훈련 시간의 극적인 감소였습니다. 트랜스포머는 이전 최첨단 모델보다 훨씬 적은 컴퓨팅 리소스를 필요로 하면서도 우수한 품질을 달성했습니다. 기본 모델은 단 12시간 만에 훈련되어 어텐션 기반 아키텍처의 효율성 이점을 입증했습니다.

저자들은 서로 다른 구성 요소의 기여를 이해하기 위해 광범위한 절제 연구를 수행했습니다. 그들은 멀티 헤드 어텐션이 단일 헤드 변형보다 훨씬 우수하며, 8개의 헤드가 최적의 성능을 제공한다는 것을 발견했습니다. 어텐션 키의 차원은 모델 품질에 중요했으며, 드롭아웃은 과적합을 방지하는 데 필수적이었습니다.

일반화 능력을 시연하기 위해 저자들은 트랜스포머를 영어 구성 구문 분석에 적용했으며, 최소한의 작업별 수정으로 경쟁력 있는 결과를 달성했습니다. 이러한 성공은 이 아키텍처가 기계 번역을 넘어 일반적인 목적의 시퀀스 모델링 프레임워크로 사용될 수 있음을 시사했습니다.

## 어텐션 시각화 및 해석 가능성

이 논문에는 모델의 동작에 대한 통찰력을 제공하는 어텐션 시각화가 포함되어 있습니다. 이 시각화는 서로 다른 어텐션 헤드가 문장 내에서 지역적인 구문 종속성에서부터 장거리 의미론적 연결에 이르기까지 서로 다른 유형의 관계에 어떻게 집중하는지를 보여줍니다.

![Attention Visualization](https://paper-assets.alphaxiv.org/figures/1706.03762v7/x1.png "Attention Visualization")_그림 4: 모델이 특정 토큰을 처리할 때 입력 시퀀스의 다른 부분에 어떻게 집중하는지를 보여주는 어텐션 시각화 예시._

어텐션 패턴은 모델이 대명사 해소, 구문 분석, 의미역 라벨링과 같은 다양한 언어 작업을 이러한 작업에 대한 명시적인 감독 없이 암묵적으로 수행하는 방법을 학습한다는 것을 보여줍니다.

## 영향과 중요성

트랜스포머 아키텍처는 어텐션 메커니즘만으로 순환 및 컨볼루션 방식에 비해 우수한 성능을 달성할 수 있음을 입증함으로써 시퀀스 모델링에 있어 패러다임의 전환을 의미합니다. 긴 범위의 의존성에서도 강력한 성능을 유지하면서 시퀀스를 병렬로 처리하는 능력은 이전 아키텍처를 제약했던 주요 한계를 해결합니다.

이 작업의 중요성은 즉각적인 결과 이상으로 확장됩니다. 트랜스포머의 효율성과 효과는 훨씬 더 큰 모델을 더 광범위한 데이터셋으로 훈련하는 것을 가능하게 했으며, 자연어 처리를 변화시킨 대규모 언어 모델의 기반을 마련했습니다. 이 아키텍처의 단순성과 일반성은 언어 모델링에서 컴퓨터 비전에 이르기까지 다양한 작업과 도메인에 적용될 수 있도록 했습니다.

RNN의 순차적 병목 현상을 제거하면서 복잡한 시퀀스 작업에 필요한 모델링 능력을 유지함으로써, 트랜스포머는 신경망 아키텍처 확장의 새로운 가능성을 열었습니다. 그 도입은 딥러닝에서 어텐션 중심 시대의 시작을 알렸으며, 연구자들이 시퀀스 모델링 문제에 접근하는 방식을 근본적으로 변화시켰습니다.