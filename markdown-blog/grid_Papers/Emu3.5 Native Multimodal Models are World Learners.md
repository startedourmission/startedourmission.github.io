---
date: 2025-11-05
tags:
  - 논문
  - LLM
aliases:
  - "Emu3.5: Native Multimodal Models are World Learners"
image: "![[1-Emu.png]]"
description: 최신 멀티모달 모델의 화두는 세계를 이해하고 행동하는 모델입니다. BAAI(Beijing Academy of Artificial Intelligence)가 최근 공개한 Emu3.5는 이런 흐름을 타고 비전과 언어를 동시에 예측하는 '멀티모달 월드 모델'을 표방하며 장시간 순차적 추론과 실제 로봇 조작까지 가능하게 한다고 주장합니다.
---

![[1-Emu.png]]

> Y. Cui, H. Chen, H. Deng, X. Huang, et al., "Emu3.5: Native Multimodal Models are World Learners," arXiv:2510.26583, 2025.

최신 멀티모달 모델의 화두는 세계를 이해하고 행동하는 모델입니다. BAAI(Beijing Academy of Artificial Intelligence)가 최근 공개한 Emu3.5는 이런 흐름을 타고 비전과 언어를 동시에 예측하는 '멀티모달 월드 모델'을 표방하며 장시간 순차적 추론과 실제 로봇 조작까지 가능하게 한다고 주장합니다.

눈에 띄는 것은 **Discrete Diffusion Adaptation (DiDA)** 기법으로 추론 속도를 20배 단축하면서도 성능을 유지했다는 점입니다. 이것이 실제로 가능할까요? 논문을 자세히 살펴보겠습니다.

## 요약

- **모델**: 34.1B 파라미터 트랜스포머 기반 
- **학습 데이터**: 10조 개 토큰 (주로 인터넷 비디오의 연속 프레임과 자막) 
- **평가 지표**: TIIF-Bench, LeX-Bench, GenEval, DPG-Bench, OneIG-Bench, ImgEdit, ICE-Bench 등 다중 벤치마크 
- **훈련 방식**: 2단계 프리트레이닝 + SFT + 대규모 멀티태스크 RL + DiDA 적응

핵심은 하나의 통합된 **다음-토큰 예측(next-token prediction)** 목표로 비전과 언어를 동시에 학습한다는 것입니다. 그 결과 텍스트-투-이미지(T2I), 이미지 편집(X2I), 비주얼 스토리텔링, 로봇 조작 등 다양한 작업을 하나의 모델로 처리할 수 있게 되었습니다.

---

## 논문 상세

### 1. 서론

기존 멀티모달 모델들은 주로 짧은 비디오 클립이나 고립된 이미지-텍스트 쌍에 기반했습니다. 하지만 인간은 어떻게 배울까요? 긴 시간에 걸친 비디오를 보면서 인과 관계를 파악하고 환경과 상호작용합니다.

Emu3.5는 이 지점에서 아이디어를 캐치합니다. 비디오의 연속 프레임과 자막을 활용해 시간적 연속성과 멀티모달 정렬을 자연스럽게 학습하는 것입니다. 이것이 진정한 '월드 모델'이 되는 첫 단계라는 주장입니다.

### 2. 데이터

단순히 "10조 토큰"이라고 해도 인상적이지만 **어떤 데이터**인지가 핵심입니다.

#### 2.1 비디오 인터리빙 데이터

약 63백만 개의 비디오 (평균 6.5분)에서 정교한 처리를 수행합니다: PySceneDetect로 장면 분할, 각 장면마다 키프레임 추출 (평균 0.27프레임/초), Whisper-large-v2로 자동 음성 인식 후 타임스탬프 정렬.

이렇게 하면 단순히 비디오 프레임만 나열하는 것이 아니라 **시간에 맞춰 정렬된 텍스트-이미지 시퀀스**가 됩니다. 이 과정이 원본 데이터의 품질을 크게 좌우합니다.

#### 2.2 필터링 파이프라인

여기서 돋보이는 것은 "기본 필터링" + "고급 필터링" 2단계입니다.

**기본 필터링**: 영상 길이, 해상도, 토킹헤드 비디오 제외, 언어 및 침묵 균형

**고급 필터링**: DeQA 모델로 프레임 품질 평가 → DINO/FG-CLIP 피처로 중복도 제거 → LLM으로 텍스트 품질 평가

이 정도면 거의 학술 논문 데이터 큐레이션 수준입니다.

#### 2.3 주석

2단계 프리트레이닝에서는 단순 자막을 넘어: 의미론적 세그멘테이션 및 요약 (LLM으로 생성), 비주얼 캡셔닝 (Qwen2.5-VL로 각 장면 설명), 멀티모달 요약 (자막 + 텍스트 세그먼트 + 캡션을 통합).

10조 토큰이지만, 오히려 "데이터가 많으면 된다" 보다는 **각 데이터 샘플의 질**에 신경쓴 것처럼 보입니다.

### 3. 단순한 아키텍처

Emu3.5는 표준 트랜스포머 기반이지만 몇 가지 최신 기법을 추가합니다:

$$\text{Parameters: } 34.1\text{B} \text{ (Transformer: } 31.2\text{B, Embedding: } 2.9\text{B)}$$

**GQA (Grouped Query Attention)**: 키-밸류 캐시를 줄여서 메모리 효율성 개선 **QK-Norm**: 쿼리와 키 프로젝션에 정규화로 어텐션 안정성 강화 **RoPE (Rotary Positional Embedding)**: 위치 인코딩

어텐션 헤드 수도 64개인데, 이 중 KV 헤드는 8개만 사용하는 구조입니다. 파라미터는 절약하되 성능 손실을 최소화하는 설계입니다.

### 4. 토크나이저

시각 토크나이저는 **IBQ (Index Backpropagation Quantization)** 프레임워크 기반입니다.

다운샘플링 비율: 16배 (512×512 이미지 → ~1024 토큰), 코드북 크기: 131,072, 모델 크기: 455M 파라미터.

특징적인 것은 **Diffusion 기반 이미지 디코더**를 추가한 점입니다. 바닐라 디코더보다 2배 높은 해상도로 복원하면서도 LoRA 기반 증류로 추론을 10배 가속화했습니다. 특히 텍스트와 얼굴 영역에서 세밀한 디테일이 살아납니다.

### 5. 학습

#### 5.1 프리트레이닝 (2단계)

**Stage 1**: 10조 토큰 (기본 필터링만, 주석 없음) - 학습률: 5e-4, 시퀀스 길이: 32K, 이미지 해상도: 최대 512×512

**Stage 2**: 3조 토큰 (고급 필터링 + 풍부한 주석) - 학습률: 1e-5, 이미지 해상도: 512-1024×1024 (동적)

여기서 눈여겨볼 점은 Stage 1과 2의 **전략적 차이**입니다. 처음에는 규모를 중시하고 두 번째 스테이지에서는 품질과 주석을 강화합니다.

#### 5.2 포스트트레이닝

**SFT (Supervised Fine-tuning)**: 150B 샘플로 6개 종류의 태스크 통합 학습

**대규모 멀티태스크 RL**: 보상 시스템은 일반적 보상(CLIP 유사도, VLM 정렬도) + 작업 특화 보상(OCR 정확도, 얼굴 보존, 일관성). 알고리즘: GRPO (Group Relative Policy Optimization). 보상 스케일: 4.5 → 7.1로 안정적 개선.

놀라운 것은 **서로 다른 작업들이 서로를 강화한다**는 발견입니다. T2I의 높은 충실도가 비주얼 내러티브로 자동 전이되고, 이미지 편집 능력이 비주얼 가이던스를 강화합니다.

### 6. DiDA

이것이 Emu3.5의 가장 혁신적인 부분입니다.

#### 6.1 기본 개념

자동회귀 모델의 문제점은 이미지 토큰 생성이 **순차적**이라는 것입니다. 1024×1024 이미지는 4K개 토큰이 필요한데, 이를 하나씩 생성하면 시간이 오래 걸립니다.

DiDA의 해결책: 모든 이미지 토큰을 **한번에 초기화** (노이즈 추가), 그 다음 여러 스텝에서 **양방향으로** 정제 (denoising).

Attention Mask 구조: 노이즈 토큰은 이전의 깨끗한 토큰에 **인과성(causal)** 적용, 노이즈 토큰끼리는 같은 이미지 내에서 **양방향** 어텐션.

이렇게 하면 **병렬 처리**가 가능해집니다.

#### 6.2 성과

|모델|파라미터|해상도|토큰 수|추론 시간|
|---|---|---|---|---|
|Emu3|8B|720×720|8,100|260s|
|Emu3.5 (AR)|34B|1024×1024|4,096|512s|
|Emu3.5 (DiDA)|34B|1024×1024|4,096|**22s**|

무려 **20배 가속**이면서도 생성 품질을 유지합니다. 이것은 진정한 엔지니어링의 승리입니다.

### 7. 평가

#### 7.1 텍스트-투-이미지 (T2I)

TIIF, LeX-Bench, LongText-Bench, CVTG-2K 등 벤치마크에서: 복잡한 명령 따르기 89.48 (TIIF 최고), 영어 텍스트 렌더링 98% 정확도 (LeX-Bench), 중국어 텍스트 92.8% (LongText-Bench).

특히 T2I 성능이 **Gemini 2.5 Flash Image와 비교 가능**하다는 것이 중요합니다. 오픈 모델 중에서 이 정도면 SOTA급입니다.

#### 7.2 이미지 편집 (Any-to-Image, X2I)

ImgEdit, GEdit-Bench, ICE-Bench 등에서 편집 지시 준수도, 일관성, 미학적 품질 모두 최고 점수입니다. Qwen-Image-Edit-2509와 Gemini 2.5를 능가합니다.

#### 7.3 비주얼 내러티브

텍스트 프롬프트나 이미지-텍스트 시퀀스로부터 **문맥 있는 스토리**를 생성합니다. 프레임 간 시각적 일관성 유지, 역사 사건/SF/판타지 등 다양한 장르 처리, ChatGPT 자동 평가에서 Gemini 2.5와 비슷한 수준.

#### 7.4 비주얼 가이던스

"요리 순서", "DIY 튜토리얼" 같은 절차적 지시를 이미지+텍스트로 생성합니다. 각 단계의 논리적 일관성 유지, 작업 완료도 높음.

#### 7.5 월드 탐색

사용자 프롬프트에 따라 가상 환경을 **상호작용적으로** 탐색합니다. 순차적 시점 변경 (카메라 움직임), 공간 일관성 유지, 자유 탐색 모드에서 세계를 자율적으로 탐색.

#### 7.6 구체화된 조작 (Embodied Manipulation)

로봇 팔이 옷을 접거나 책상을 정리하는 작업을 **세부 단계별**로 생성합니다. 물리 법칙 준수, 장면 배경 일관성, 서로 다른 로봇 아키텍처(Aloha, Widow X 등)에 일반화 가능.

---

## 핵심 통찰

1. 스케일만으로는 부족합니다

단순히 토큰 10조를 쏟아 붓는 것이 아니라 각 데이터 샘플의 **질**을 극도로 정제했습니다. 이것이 일반화 성능을 결정하는 핵심입니다.

2. 통합 학습이 상승 효과를 냅니다

6가지 상이한 태스크를 하나의 보상 시스템으로 RL 학습하니 T2I의 높은 충실도가 비주얼 내러티브로 자동 전이되고, 편집 능력이 가이던스 생성을 강화합니다. 단일 작업 최적화보다 나은 일반화 성능입니다.

3. 추론 효율은 모델 설계에서 시작됩니다

DiDA는 단순한 트릭이 아니라 자동회귀 모델 자체를 양방향 디코딩으로 재해석한 것입니다. 학습-추론 패러다임의 유연성을 보여줍니다.

4. 멀티모달은 이제 선택 사항이 아닙니다

T2I, 편집, 스토리텔링, 로봇 조작까지 하나의 모델로 처리하는 것이 현실이 되었습니다. 앞으로는 각 작업마다 모델을 분리해서 개발하는 것이 비효율적으로 보일 수도 있습니다.

---

## 결론

Emu3.5는 재밌는 논문입니다. 10조 토큰의 규모도 중요하지만 데이터 큐레이션, 통합 포스트트레이닝, 효율적인 추론 기법이 더 큰 역할을 한다는 것이 핵심 메시지입니다.

DeepSeek-R1처럼 추론 모델이 각광받는 와중에 멀티모달 월드 모델이라는 메시지가 신선합니다. 로봇 조작, 장시간 추론, 환경 상호작용까지 아우르는 모델이 등장했다는 것은 AI의 실제 애플리케이션이 대폭 확대될 수 있음을 시사합니다.

오픈소스로 공개한 것도 좋네요. 중국은 뭐 항상 오픈소스네요. 정말 미친 것 같습니다.

---

**논문 링크**: https://arxiv.org/abs/2510.26583  
**GitHub**: https://github.com/baaivision/Emu3.5