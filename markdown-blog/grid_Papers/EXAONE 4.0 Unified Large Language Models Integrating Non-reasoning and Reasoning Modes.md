---
date: 2025-08-10
tags:
aliases:
  - 엑사원 4.0
image: "![[exaone_1.png]]"
---
LG AI Research가 공개한 EXAONE 4.0은 언어모델 설계에서 새로운 접근을 시도했습니다. Non-reasoning 모드와 Reasoning 모드를 하나의 모델에 통합하여, 일상적인 대화에서는 빠른 응답을, 복잡한 문제에서는 깊은 사고를 제공하는 실용적인 언어모델을 구현했습니다.

> LG AI Research, "EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes," arXiv preprint arXiv:2507.11407, 2025.

![[exaone_1.png]]
# 요약

## 1. 모델 아키텍처

### 기본 구성

|구성요소|32B 모델|1.2B 모델|
|---|---|---|
|**파라미터 수**|32.0B|1.28B|
|**모델 차원 (d_model)**|5,120|2,048|
|**레이어 수**|64|30|
|**어텐션 헤드**|40|32|
|**KV 헤드**|8|8|
|**헤드 크기**|128|64|
|**최대 시퀀스 길이**|131,072|65,536|
|**정규화**|QK-Reorder-LN|QK-Reorder-LN|
|**활성화 함수**|SwiGLU|SwiGLU|
|**FFN 차원**|27,392|4,096|

### 어텐션 메커니즘

|모델|어텐션 타입|글로벌:로컬 비율|윈도우 크기|RoPE 적용|
|---|---|---|---|---|
|**32B**|Hybrid|1:3|4K 토큰|로컬만|
|**1.2B**|Global|-|-|전체|

### 토크나이저 및 어휘

- **타입**: BBPE (Byte-level BPE)
- **어휘 크기**: 102,400
- **구성**: 한국어, 영어 토큰 동일 비율 + 소량 다국어 토큰
- **임베딩 공유**: 32B (False), 1.2B (True)
- **RoPE theta**: 1,000,000

## 2. 훈련 데이터

### 사전훈련 규모

|모델 크기|훈련 토큰 수|계산량 (FLOPs)|지식 컷오프|
|---|---|---|---|
|**32B**|14T|2.69 × 10²⁴|2024년 11월|
|**1.2B**|12T|8.65 × 10²²|2024년 11월|

**비교**: EXAONE 3.5 32B는 6.5T 토큰 (2배 이상 증가)

### 도메인별 데이터 구성

#### 1. World Knowledge

- **소스**: 웹 기반 교육적 가치 데이터
- **필터링**: 교육적 가치 기준으로 고품질 데이터 선별
- **특화**: 고난도 전문 데이터로 Reasoning 모드 강화

#### 2. Math/Code/Logic

- **전략**: 검증 가능한 답변에 대한 다양한 응답 생성
- **품질 관리**: 언어 일관성 확보를 위한 신중한 필터링
- **확장**: 코딩 문제 해결 + 풀스택 개발 소프트웨어 엔지니어링

#### 3. Long Context

- **구성**: 웹 코퍼스 기반 포괄적 이해 요구 태스크
- **변화**: 맥락 길이와 핵심 내용 위치를 체계적으로 변화
- **한국어**: 법률, 행정, 기술 문서 재구성 및 다양한 입력 형식

#### 4. Agentic Tool Use

- **초점**: 단순 도구 호출보다 복잡한 장기 호라이즌 데이터
- **구성**: 사용자-에이전트 대화 + 실행 피드백 + 반복적 추론
- **형태**: 멀티스텝, 멀티턴 형식으로 구성

#### 5. Multilinguality

- **언어**: 한국어, 영어, 스페인어 (새로 추가)
- **내용**: 각 언어별 문화적, 역사적 지식 특화
- **한국어**: 교육, 산업 전문가 수준 도메인 특화 데이터

### 통합 모드 훈련 설정

- **토큰 비율**: Reasoning 1.5 : Non-Reasoning 1
- **훈련 방식**: 순차가 아닌 동시 통합 훈련
- **2차 훈련**: Code, Tool Use 도메인 고품질 Reasoning 데이터 재사용

## 3. 맥락 길이 확장

### 점진적 확장 전략

- **1단계**: 4K → 32K 토큰
- **2단계**: 32K → 128K 토큰 (32B), 64K 토큰 (1.2B)

### 검증 방법

- **NIAH 테스트**: 각 단계별 철저한 성능 검증
- **"Green Light" 신호**: 모든 구간에서 일관된 성능 확인 시까지 반복

### 데이터 선별 및 훈련 레시피

- 신중한 데이터 선별 방법론 적용
- 점진적 훈련 레시피로 효율성과 성능 균형

## 4. 포스트 트레이닝

### 3단계 파이프라인

1. **대규모 SFT**: 통합 모드 지도 파인튜닝
2. **추론 RL**: AGAPO 알고리즘 적용
3. **선호학습**: 하이브리드 리워드로 모드 통합

### AGAPO 알고리즘 세부사항

#### 데이터 준비

- **카테고리**: 수학, 코딩, 과학, 지시 따르기
- **정확도 기반 필터링**: 8개 응답 모두 정답인 쉬운 문제 제외
- **응답 생성**: 문제당 8개 응답 생성 후 필터링

#### 리워드 함수 (도메인별)

- **수학**: 규칙 기반 검증기
- **코딩**: 테스트 케이스 통과 여부
- **과학**: 규칙 기반 + LLM 판사 결합
- **지시 따르기**: 제약 조건 만족 시 1, 아니면 0

#### 핵심 개선사항

1. **클립 목적함수 제거**: 표준 정책 그래디언트 손실 사용
2. **비대칭 샘플링**: 모든 틀린 응답도 네거티브 피드백으로 활용
3. **2단계 어드밴티지**: 그룹 내 LOO → 배치 전체 정규화
4. **시퀀스 레벨 누적 KL**: SFT 능력 보존

#### AGAPO 목적함수

```
J_AGAPO(θ) = E[1/G * Σ(A_global,i * log π_θ(o_i|q) - β * D_KL(π_θ, π_ref))]

A_loo,i = r_i - (1/(G-1)) * Σ(j≠i) r_j
A_global,i = (A_loo,i - mean({A_loo,k})) / std({A_loo,k})
```

### 선호학습 (2단계)

#### 1단계: 토큰 효율성 개선

- **리워드 조합**: 검증 리워드 + 간결성 리워드
- **선택 기준**: 정답 중 가장 짧은 응답
- **대상**: 추론 관련 검증 가능 데이터

#### 2단계: 인간 정렬

- **리워드 조합**: 선호 리워드 + 언어 일관성 리워드
- **평가 범위**: Reasoning 모드에서 최종 답변만
- **안정성**: 1단계 데이터 일부 재사용

#### 데이터 구성

- **응답 생성**: 쿼리당 4-16개 응답
- **선택 기준**: 하이브리드 리워드 기반 선택/거부 쌍 구성
- **방법**: SimPER 방식의 reference-free 선호 최적화

## 5. 평가 설정

### 벤치마크 카테고리 (6개)

#### World Knowledge

- **MMLU-Redux**: 개선된 MMLU, 오염 문제 해결
- **MMLU-Pro**: 더 어려운 MMLU 확장판
- **GPQA-Diamond**: 대학원 수준 생물학, 물리학, 화학

#### Math/Coding

- **AIME 2025**: 미국 수학 초청 시험
- **HMMT Feb 2025**: 하버드-MIT 수학 토너먼트
- **LiveCodeBench v5/6**: 실시간 코딩 평가 (오염 방지)

#### Instruction Following

- **IFEval**: 지시 따르기 능력 평가
- **Multi-IF**: 멀티턴, 다국어 지시 따르기 (영어 서브셋만 사용)

#### Long Context

- **HELMET**: 합성 + 실제 장문 태스크 (LongCite 제외)
- **RULER**: 다양한 장문 이해 능력 평가
- **LongBench**: 이중언어 장문 벤치마크 (영어 서브셋)

#### Agentic Tool Use

- **BFCL-v3**: 함수 호출 능력 종합 평가
- **Tau-Bench**: 시뮬레이션 대화 기반 도구 사용 (gpt-4.1-2025-04-14 사용자 역할)

#### Multilinguality

- **한국어**:
    - KMMLU-Pro/Redux (KMMLU 대신, 오염/오류 문제 해결)
    - KSM (한국 수학): HRM8K의 고등학교-올림피아드 수준
    - Ko-LongBench (자체 개발): 의료, 법률, 금융, 특허 등
- **스페인어**:
    - MMMLU (es), MATH500 (es)
    - WMT24++: LLM-as-a-Judge로 번역 품질 평가

### 하이퍼파라미터 설정

#### 샘플링 설정

|벤치마크|샘플 수 (n)|용도|
|---|---|---|
|GPQA-Diamond|8|안정성 확보|
|AIME 2025, HMMT|32|수학 경쟁|
|LiveCodeBench, Tau-Bench|4|코딩/도구|
|MATH500 (es)|4|다국어 수학|

#### 모드별 설정

|모드|Temperature|Top-p|Presence Penalty|토큰 제한|
|---|---|---|---|---|
|**Reasoning**|0.6|0.95|1.5 (32B만)|64K (수학/코딩), 32K (기타)|
|**Non-Reasoning**|0.0 (Greedy)|-|0.0|동일|

### 평가 해상도

- **1024 패치 토큰 기준**:
    - 패치 14: 448×448px
    - 패치 16: 512×512px

## 6. 기준 모델 비교

### 모델 분류

- **Small-size**: 3B 미만
- **Mid-size**: 10B-30B
- **Frontier**: 200B 이상

### 타입별 분류

- **Non-Reasoning**: CoT 스타일 생성
- **Reasoning**: 긴 CoT 스타일 생성
- **Hybrid**: 모드별 선택적 생성

# 논문 상세

## 핵심 특징: 하이브리드 모드 아키텍처

### 1. 이중 모드 시스템

EXAONE 4.0의 가장 독특한 특징은 하나의 모델에서 두 가지 동작 모드를 제공한다는 점입니다:

**Non-reasoning 모드**:

- 빠른 응답이 필요한 일반적인 대화와 질문에 최적화
- CoT(Chain-of-Thought) 스타일의 간결한 추론
- Greedy decoding 또는 낮은 temperature 설정으로 안정적 출력

**Reasoning 모드**:

- 수학, 코딩, 과학적 추론이 필요한 복잡한 문제에 특화
- `<think>` 블록을 활용한 긴 사고 과정 표현
- 최대 64K 토큰의 추론 공간 제공
- Temperature 0.6, top-p 0.95로 창의적 탐색 허용

### 2. 모델 구성

|모델|파라미터|차원|레이어|헤드|최대 길이|적용|
|---|---|---|---|---|---|---|
|**32B**|32.0B|5,120|64|40|131,072|고성능 서버|
|**1.2B**|1.28B|2,048|30|32|65,536|온디바이스|

## 아키텍처 혁신

### 1. 하이브리드 어텐션 메커니즘

32B 모델에서는 계산 효율성과 성능의 균형을 위해 혁신적인 하이브리드 어텐션을 도입:

**설계 원리**:

- 로컬 어텐션(슬라이딩 윈도우)과 글로벌 어텐션을 **3:1 비율**로 결합
- 슬라이딩 윈도우 크기: **4K 토큰**
- 글로벌 어텐션에서는 **RoPE 사용하지 않아** 길이 편향 제거
- 청크 어텐션 대신 **슬라이딩 윈도우 방식** 채택으로 이론적 안정성 확보

**성능 효과**:

- 단문 맥락 성능 저하 최소화
- 장문 맥락 처리 시 계산 비용 절감
- 최대 128K 토큰(32B), 64K 토큰(1.2B)까지 안정적 처리

### 2. QK-Reorder-LN 정규화

기존 Pre-LN 구조의 한계를 극복하기 위한 정규화 위치 재조정:

- LayerNorm을 어텐션과 MLP 출력에 직접 적용
- Q, K 프로젝션 후 **RMS 정규화** 추가
- 계산량 증가에도 불구하고 다운스트림 성능 향상

## 대규모 훈련 전략

### 1. 데이터 스케일 혁신

**사전훈련 규모**:

- **32B 모델**: 14T 토큰 (EXAONE 3.5 대비 2배 증가)
- **1.2B 모델**: 12T 토큰
- **계산량**: 32B는 2.69×10²⁴ FLOPs

**점진적 맥락 확장**:

- **1단계**: 4K → 32K 토큰
- **2단계**: 32K → 128K 토큰 (32B), 64K 토큰 (1.2B)
- 각 단계에서 **NIAH 테스트**로 성능 검증

### 2. 5개 도메인 특화 데이터

**1. World Knowledge**:

- 웹 소스 기반 교육적 가치 데이터 필터링
- 고난도 전문 데이터로 Reasoning 모드 강화

**2. Math/Code/Logic**:

- 검증 가능한 답변에 대한 **다양한 응답 생성** 전략
- 풀스택 개발 중심 소프트웨어 엔지니어링 데이터 확장

**3. Long Context**:

- 맥락 길이와 핵심 내용 위치를 체계적으로 변화
- 한국어: 법률, 행정, 기술 문서 재구성

**4. Agentic Tool Use**:

- 단일턴보다 **복잡한 장기 도구 호출** 데이터에 집중
- 사용자-에이전트 대화, 실행 피드백, 반복적 추론 포함

**5. Multilinguality**:

- **스페인어** 공식 지원 추가 (영어, 한국어와 함께)
- 문화적, 역사적 지식 특화 데이터

### 3. 통합 모드 훈련

**핵심 전략**:

- **토큰 비율**: Reasoning 1.5 : Non-reasoning 1
- 순차 훈련이 아닌 **동시 훈련**으로 모드 통합
- 2차 훈련에서 Code, Tool Use 도메인 재샘플링

## AGAPO: 혁신적 강화학습 알고리즘

기존 GRPO의 한계를 극복한 **Asymmetric Sampling and Global Advantage Policy Optimization**:

### 핵심 개선사항

**1. 클립 목적함수 제거**:

- PPO의 클립 손실 대신 표준 정책 그래디언트 사용
- 저확률 탐색 토큰의 그래디언트 기여 허용

**2. 비대칭 샘플링**:

- 모든 응답이 틀린 샘플도 **네거티브 피드백**으로 활용
- 기존 GRPO가 버리던 데이터 효과적 활용

**3. 2단계 어드밴티지 계산**:

```
A_loo,i = r_i - (1/(G-1)) * Σ(r_j), j≠i
A_global,i = (A_loo,i - mean({A_loo,k})) / std({A_loo,k})
```

**4. 시퀀스 레벨 누적 KL**:

- SFT 단계에서 학습한 능력 보존

### 목적함수

```
J_AGAPO(θ) = E[1/G * Σ(A_global,i * log π_θ(o_i|q) - β * D_KL(π_θ, π_ref))]
```

## 2단계 선호학습

**1단계: 토큰 효율성 개선**:

- **검증 리워드 + 간결성 리워드** 조합
- 정답 중 가장 짧은 응답 선택

**2단계: 인간 정렬**:

- **선호 리워드 + 언어 일관성 리워드** 조합
- Reasoning 모드에서 최종 답변만 선호 라벨링

## 포괄적 성능 평가

### 수학/코딩 도메인 압도적 우위

**32B 모델 성과**:

- **AIME 2025**: 85.3% (Reasoning), 35.9% (Non-reasoning)
- **HMMT Feb 2025**: 72.9% (Reasoning), 21.8% (Non-reasoning)
- **LiveCodeBench v6**: 66.7% (Reasoning), 43.1% (Non-reasoning)

**주목할 성과**: Qwen3 235B (7배 큰 모델) 대비 모든 수학/코딩 벤치마크에서 우수

### 세계 지식과 전문 추론

**GPQA-Diamond** (대학원 수준 과학):

- 75.4% (Reasoning), 63.7% (Non-reasoning)
- 같은 클래스 모델 중 **2위 성능**

**MMLU-Redux** (개선된 MMLU):

- 92.3% (Reasoning), 89.8% (Non-reasoning)
- 대폭 증가한 훈련 데이터의 효과 입증

### 에이전트 능력과 도구 사용

**BFCL-v3** (함수 호출):

- 63.9% (Reasoning), 65.2% (Non-reasoning)

**Tau-Bench**:

- Airline: 51.5% (Reasoning), 25.5% (Non-reasoning)
- Retail: 62.8% (Reasoning), 55.9% (Non-reasoning)

### 다국어 성능

**한국어**:

- KMMLU-Pro: 67.7% (Reasoning), 60.0% (Non-reasoning)
- KSM: 87.6% (Reasoning), 59.8% (Non-reasoning)
- Ko-LongBench: 76.9% (Non-reasoning)

**스페인어** (새로 추가):

- MMMLU: 85.6% (Reasoning), 80.6% (Non-reasoning)
- MATH500: 95.8% (Reasoning), 87.3% (Non-reasoning)
- WMT24++ 번역: 90.7% (Non-reasoning)

## 추론 예산 분석

추론 토큰 수에 따른 성능 변화 분석:

|예산|32B AIME 2025|32B LiveCodeBench v6|1.2B AIME 2025|1.2B LiveCodeBench v6|
|---|---|---|---|---|
|**64K**|85.3%|66.7%|45.2%|45.3%|
|**32K**|74.8% (-12.3%)|67.3% (+0.9%)|45.3% (+0.2%)|43.0% (-5.1%)|
|**16K**|44.2%|53.0%|37.1%|40.1%|
|**8K**|36.8%|47.6%|24.6%|38.3%|

**실용적 의미**: 32K 예산으로도 대부분 벤치마크에서 경쟁력 유지

## 장문 맥락 처리 능력

### HELMET 벤치마크 (128K)

**32B 모델**: 58.3% 평균 (6개 태스크)

- Synthetic Recall: 94.1%
- RAG: 54.8%
- LongQA: 52.3%

**1.2B 모델**: 42.5% 평균 (64K)

- 소형 모델치고 뛰어난 장문 처리 능력

### RULER 벤치마크

**128K 성능**: 88.2% (32B), 77.4% (1.2B)

- 해상도 증가에 따른 안정적 성능 유지

## 기술적 의의와 혁신

### 1. 실용적 하이브리드 설계

단순한 성능 추구가 아닌 **실제 사용 시나리오**에 중점:

- 사용자가 상황에 따라 모드 선택 가능
- 계산 효율성과 성능의 실용적 균형

### 2. 한국어 생태계 강화

**전문 지식 처리**:

- KMMLU-Pro/Redux로 검증된 전문가 수준 성능
- 교육, 산업 도메인 특화 데이터 큐레이션

### 3. 에이전트 AI 준비

**도구 사용 능력**:

- 복잡한 멀티턴 상호작용 지원
- 실행 피드백 기반 반복적 추론

### 4. 모델 패밀리 전략

**효율적 증류**:

- 70억 파라미터 교사에서 다양한 크기 학생 모델 생성
- 12억 모델도 준수한 성능으로 온디바이스 활용 가능

## 한계와 향후 과제

### 현재의 제약사항

**기술적 한계**:

- 추론 모드에서 언어 일관성 위험
- 모드 전환 시 일관성 유지 도전
- 대규모 모델의 계산 자원 요구

**데이터 의존성**:

- 훈련 데이터 통계에 기반한 생성의 한계
- 최신 정보 반영 제한 (2024년 11월 컷오프)

### 개선 방향

1. **언어 지원 확대**: 점진적 다국어 확장
2. **효율성 최적화**: 추론 모드 계산 비용 절감
3. **모드 통합 개선**: 더 자연스러운 전환 메커니즘

## 결론

EXAONE 4.0은 학술적 벤치마크 점수 경쟁보다는 실제 활용성에 중점을 둔 모델입니다. 하이브리드 모드 아키텍처, AGAPO 강화학습, 효율적인 어텐션 메커니즘을 통해 다양한 사용 시나리오에 대응할 수 있는 실용적 해법을 제시했습니다.

특히 한국어 지원 강화와 에이전트 능력 구축을 통해 실제 비즈니스 환경에서의 활용 가능성을 높였으며, 모델 패밀리 전략으로 서버에서 모바일까지 다양한 배포 환경을 지원합니다. 무엇보다 사용자가 필요에 따라 빠른 응답과 깊은 추론을 선택할 수 있다는 점이 실용적 가치를 더합니다.
