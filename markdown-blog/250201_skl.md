# scikit-learn

### 1. 데이터 전처리 (Preprocessing)
```python
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 특성 스케일링
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 정규화
normalizer = preprocessing.Normalizer()
X_normalized = normalizer.fit_transform(X)

# 레이블 인코딩
label_encoder = preprocessing.LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# 원핫 인코딩
onehot_encoder = preprocessing.OneHotEncoder()
y_onehot = onehot_encoder.fit_transform(y.reshape(-1, 1))
```

### 2. 모델 선택 및 학습
```python
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

# 선형 회귀
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# 로지스틱 회귀
logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)

# 결정 트리
tree_model = DecisionTreeClassifier(max_depth=5)
tree_model.fit(X_train, y_train)

# 랜덤 포레스트
rf_model = RandomForestClassifier(n_estimators=100)
rf_model.fit(X_train, y_train)

# SVM
svm_model = SVC(kernel='rbf')
svm_model.fit(X_train, y_train)
```

### 3. 모델 평가
```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    mean_squared_error, r2_score, confusion_matrix
)

# 예측
y_pred = model.predict(X_test)

# 분류 평가 지표
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# 회귀 평가 지표
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# 혼동 행렬
conf_matrix = confusion_matrix(y_test, y_pred)
```

### 4. 교차 검증
```python
from sklearn.model_selection import (
    cross_val_score, KFold, GridSearchCV
)

# K-폴드 교차 검증
kfold = KFold(n_splits=5, shuffle=True)
scores = cross_val_score(model, X, y, cv=kfold)

# 그리드 서치
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, 30]
}
grid_search = GridSearchCV(model, param_grid, cv=5)
grid_search.fit(X_train, y_train)
```

### 5. 특성 선택과 차원 축소
```python
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.decomposition import PCA

# 특성 선택
selector = SelectKBest(chi2, k=5)
X_selected = selector.fit_transform(X, y)

# PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
```

### 6. 파이프라인
```python
from sklearn.pipeline import Pipeline

# 파이프라인 구축
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=2)),
    ('classifier', RandomForestClassifier())
])

# 파이프라인 실행
pipeline.fit(X_train, y_train)
```

### 7. 군집화
```python
from sklearn.cluster import KMeans, DBSCAN

# K-평균 군집화
kmeans = KMeans(n_clusters=3)
clusters = kmeans.fit_predict(X)

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
clusters = dbscan.fit_predict(X)
```

### 8. 텍스트 처리
```python
from sklearn.feature_extraction.text import (
    CountVectorizer, TfidfVectorizer
)

# 카운트 벡터화
count_vec = CountVectorizer()
X_count = count_vec.fit_transform(texts)

# TF-IDF 벡터화
tfidf_vec = TfidfVectorizer()
X_tfidf = tfidf_vec.fit_transform(texts)
```

### 9. 모델 저장 및 로드
```python
from sklearn.externals import joblib

# 모델 저장
joblib.dump(model, 'model.pkl')

# 모델 로드
loaded_model = joblib.load('model.pkl')
```

### 10. 앙상블 방법
```python
from sklearn.ensemble import (
    VotingClassifier, BaggingClassifier, AdaBoostClassifier
)

# 보팅
voting_clf = VotingClassifier(
    estimators=[
        ('rf', RandomForestClassifier()),
        ('svm', SVC())
    ]
)

# 배깅
bagging = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(),
    n_estimators=10
)

# 부스팅
boosting = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(),
    n_estimators=50
)
```

